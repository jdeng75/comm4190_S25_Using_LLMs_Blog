<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jenn Deng">
<meta name="dcterms.date" content="2025-03-25">
<meta name="description" content="wwowowow this is exciting">

<title>Playing Around with Gemini 2.0 – Jenn Deng’s Exploration with LLMs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-20bec5dd6b4765711cbca3fd2df8130c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Jenn Deng’s Exploration with LLMs</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Playing Around with Gemini 2.0</h1>
                  <div>
        <div class="description">
          wwowowow this is exciting
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">LLMs</div>
                <div class="quarto-category">prompting</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Jenn Deng </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 25, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="gemini-2.0-flash-experimental" class="level1">
<h1>Gemini 2.0 Flash Experimental !!!</h1>
<p>The class recently got an announcement about Google’s latest multi-modal text to image model, <strong>Gemini 2.0 Flash Experimental.</strong> <br> The model can edit images on the gly without generating them entirely new, which is crazy. <br> Around the same time, I also stumbled upon a <strong>Tik Tok video by <span class="citation" data-cites="hes_a_nerd">@hes_a_nerd</span>.</strong></p>
<p><img src="1.png" width="100%"></p>
<p>At first, he explains how this photo looks like a normal photo taken with a group of people… but it’s actually <strong>ChatGPT 4.0s</strong> latest ability with photos. It can now generate photos with direct prompting and it’s way <em>better than DALLE.</em> <br> Here’s their statement about this new update: <br> &gt; At OpenAI, we have long believed image generation should be a primary capability of our language models. That’s why we’ve built our most advanced image generator yet into GPT‑4o. The result—image generation that is not only beautiful, but useful.</p>
<p>You might wonder, how did someone create this image. Here’s what they typed in.</p>
<pre><code>prompt: Create an image that looks like it was taken from an iPhone 6, a group of college aged kids, 3 or 4 smiling while outside. It is dusk, and they are on a city campus. It should be noisy and look authentic not cinematic, this photo was lazily taken in the November cold.</code></pre>
<p>There’s nothing that stands out about it that looks like it’s from AI. It can generate text the most flawlessly, and it can create text on the boards. There’s even a reflection and the details are insane. You can edit with one sentence. That’s crazy. Take a look.</p>
<p><img src="2.png" width="100%"></p>
<p>You can tell that in the prompt, they create specific text to have in the imaging. This is a next level, new update. <strong>It’s scary.</strong> <br> The improved capabilities are made to train models on the joint distribution of online images and text, learning not just how images relate to language, but how they relate to each other. It’s combined with aggressive post-training and is able to produce visual fluency, capable of generating images that are useful, consistent, and context-aware. <br> Text rendering is possible. 4o’s ability to blend precise symbls with imagery turns image generation into a tool for visual communication.</p>
<p>For this blog post, I want to experiment with Gemini 2.0 Flash Experimental. Let’s see if I can replicate these images above and prompt it to create realistic photos.</p>
<section id="lets-create-the-college-group-photo" class="level3">
<h3 class="anchored" data-anchor-id="lets-create-the-college-group-photo">Let’s Create the College Group Photo</h3>
<p><img src="3.png" width="100%"></p>
<p>Woah, I’ve never used this before. I like how they have examples of what you can ask it to do. But I’m going to stick to entering the prompt for the photo. Let’s see if it generates something similar to ChatGPT’s model.</p>
<p><img src="4.png" width="100%"></p>
<p>Erm…. why is this not working.</p>
<p><img src="5.png" width="100%"></p>
<p>Ok, maybe the prompt was too complicated. I’ll just… simplify it and ask: <br></p>
<pre><code>prompt: Can you create a photo of college kids on their campus.</code></pre>
<p>The output is this: <br></p>
<pre><code>{
  "prompt": "A photorealistic image of a diverse group of four college students (early 20s, mixed gender and ethnicity) sitting together casually on a sunny green lawn (quad) on a university campus. They are dressed in casual clothes like jeans, t-shirts, and hoodies, with backpacks resting beside them on the grass. One student is smiling while looking at a laptop open on their lap, while the others are engaged in relaxed conversation, looking at each other or the laptop screen. In the background, classic brick university buildings with large windows and mature trees are visible under a clear blue sky with soft sunlight. The atmosphere is bright, relaxed, and friendly. Medium shot, natural lighting.",
  "negative_prompt": "blurry, unfocused, poorly lit, distorted faces, unnatural poses, empty campus, children, high school students, business attire, overly staged",
  "style_raw": "photorealistic",
  "aspect_ratio": "16:9"
}</code></pre>
<p>What I did with this output was I inputted into ChatGPT 4o. This is what it gave me.</p>
<p><img src="6.png" width="100%"></p>
<p>This isn’t as realistic as the original. I wonder why.</p>
<p>There’s a toggle where I can click Gemini’s <strong>Thoughts</strong>, so here’s what it explains. It explains the thinking process that could lead to the image and description that I provided: <br></p>
<pre><code>Understand the Request: The user wants a photo of "college kids on their campus." This is a fairly broad request, so I need to make some assumptions or choose a common, relatable scenario.

Identify Key Elements:

Subject: College students ("kids" implies younger, undergraduate age, probably diverse).

Setting: A college campus. This needs typical campus elements.

Action/Mood: What are they doing? How do they feel? Needs to look natural and characteristic of college life.

Brainstorm Campus Scenarios:

Walking between classes.

Sitting on a quad/lawn, studying or relaxing.

Gathered near a prominent building (library, student union, academic hall).

Interacting in a group.

Carrying backpacks, books, laptops.

Laughing, talking, focused.

Choose a Specific Scenario: Sitting on the grass on a sunny day seems like a classic, positive, and visually appealing campus scene. It allows for group interaction and showcases the campus environment. Let's go with a small group relaxing and maybe chatting or looking at something together (like a laptop or phone).

Flesh out the Details for the Image Generation:

Number of students: A small group, maybe 3-5, feels natural for a casual gathering.

Diversity: Aim for a mix of genders and ethnicities to represent a typical modern campus.

Appearance: Casual college attire – jeans, t-shirts, hoodies, sneakers. Backpacks nearby. Maybe one has a laptop or notebook out.

Setting Details: Green grass (a quad or lawn). Some recognizable campus architecture in the background – perhaps brick buildings, trees, pathways. Maybe hints of other students further away to add life.

Lighting/Atmosphere: Bright, sunny day. Warm, relaxed, friendly atmosphere. Natural lighting.

Composition: Medium shot, showing the students and some of the surrounding campus. Eye-level perspective.

Formulate the Prompt for the AI Image Generator: Based on the above details, construct a prompt. It needs to be descriptive.

Initial thought: "College students on campus lawn." (Too simple)

Adding detail: "Photo of a diverse group of 4 college students (2 male, 2 female) sitting casually on a sunny green lawn on their university campus."

Adding action/mood: "Photo of a diverse group of 4 college students (early 20s) laughing and talking together, sitting casually on a sunny green quad lawn on their university campus."

Adding background/atmosphere: "Photorealistic image of a diverse group of 4 college students (early 20s, mixed gender/ethnicity) sitting together casually on a bright, sunny green lawn (quad) on a university campus. They are dressed in casual clothes (jeans, t-shirts, hoodies) and have backpacks nearby. One might be looking at a laptop. In the background, classic brick university buildings and trees are visible under a clear blue sky. Warm, relaxed, friendly atmosphere. Natural lighting." (This is getting good and detailed).

Refining for style: "Photorealistic style, natural lighting, medium shot."

Generate the Image: Use the refined prompt with the AI image generator.

Review the Output: Look at the generated image. Does it match the request and the intended details?

Are they college-aged? Yes.

Is it a campus setting? Yes, lawn, buildings.

Is the group diverse? Yes.

Is the mood right? Yes, looks relaxed/interactive.

Is the quality good? Yes, photorealistic.

Write the Description: Describe the generated image for the user, highlighting the key elements and confirming it meets their request.

Start by stating what the image shows.

Describe the subjects (students, their appearance, actions).

Describe the setting (lawn, buildings, weather).

Mention the overall mood/atmosphere.

Offer to generate variations if needed.

Final Check: Read the request, the generated image, and the description together. Does it all align? Yes.</code></pre>
<p>Well. Let’s see if it can edit current photos. I’ll use the college campus one that it generated, and I’ll add a few things here and there.</p>
</section>
<section id="attempting-to-generate-words-in-a-photo" class="level3">
<h3 class="anchored" data-anchor-id="attempting-to-generate-words-in-a-photo">Attempting to Generate Words in A Photo</h3>
<p>Let’s see if Gemini is able to create readable words in an image. <br></p>
<pre><code>Create an image of someone taking a picture of a person writing on a white board. On the white board, the title is "transfer between modalities" on the top left. Under this text, have this sentence "Suppose we directly model. Under this text, have a pros and cons. To the right, have a paragraph that says "Fixes: * model compressed representations *compose autoregressive prior with a powerful decoder.</code></pre>
<p>Here’s the output: <br></p>
<pre><code>{
  "prompt": "Photorealistic image capturing a scene from slightly behind a person holding a smartphone horizontally, taking a picture. The main focus is another person (e.g., a researcher or student) standing in front of a large, brightly lit whiteboard in a modern office or classroom setting. The person at the whiteboard is holding a black marker, perhaps pointing towards the text. The whiteboard clearly displays the following handwritten text:\n\nTop left corner: Title 'Transfer Between Modalities'\nDirectly below the title: Sentence 'Suppose we directly model.'\nBelow that sentence, a section structured for 'Pros:' and 'Cons:' (perhaps with a few placeholder bullet points or lines under each label).\nPositioned to the right of the 'Pros and Cons' section: A paragraph titled 'Fixes:' followed by two distinct bullet points (using asterisks):\n'* model compressed representations'\n'* compose autoregressive prior with a powerful decoder'\n\nThe lighting is bright and naturalistic, typical of an indoor office environment. The focus should be sharp on the whiteboard content and the person writing/presenting. The photographer's hands and phone are partially visible in the foreground framing the shot. Minimal background clutter.",
  "negative_prompt": "blurry text, illegible writing, misspelled words, incorrect text layout, distorted hands, multiple people writing, chalkboard, dark room, messy handwriting, cartoon, illustration, drawing, sketch, cluttered background, empty whiteboard",
  "style_raw": "photorealistic",
  "aspect_ratio": "16:9"
}</code></pre>
<p><img src="7.png" width="100%"></p>
<p>Well, this looks nothing like the example that was given above. It was able to create some words, but there’s still some slight mistakes with the words in the photo. The photo also doesn’t look realistic, which is also a big con.</p>
</section>
<section id="modifying-images" class="level3">
<h3 class="anchored" data-anchor-id="modifying-images">Modifying Images</h3>
<p>Another tool Gemini can be used for is creating edits of images. A friend of mine a few days ago posted “Studio Ghibli” versions of his photos. Here’s an example of what Studio Ghibli art is like: <br></p>
<div style="display: grid;grid-template-columns: repeat(auto-fill, minmax(150px, 1fr));grid-gap: 1em;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8.png" class="img-fluid figure-img" data-group="my-gallery"></p>
<figcaption>A Lovely Image</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="9.png" class="img-fluid figure-img" data-group="my-gallery"></p>
<figcaption>Another Lovely Image</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="10.png" class="img-fluid figure-img" data-group="my-gallery"></p>
<figcaption>The Last Lovely Image</figcaption>
</figure>
</div>
</div>
<p>I wonder if this will be possible using Gemini. Let’s try it out.</p>
<pre><code>{
  "prompt": "A group of five diverse college students (mixed gender and ethnicity, resembling the original photo's subjects) sitting together casually on a lush green lawn on a university campus quad, rendered in the iconic Studio Ghibli anime style. They are dressed in simple, casual clothes (jeans, t-shirts, sweaters). Backpacks, a slightly stylized laptop, and books rest beside them on the grass. They have expressive, kind faces with characteristic Ghibli eyes, smiling and engaged in relaxed conversation. In the background, charming, slightly weathered brick university buildings with large windows and hints of ivy are depicted with soft, painterly detail. The scene is bathed in warm, gentle sunlight under a vibrant blue sky with fluffy, hand-painted Ghibli clouds. The overall atmosphere is peaceful, nostalgic, and heartwarming. Hand-drawn aesthetic, vibrant but soft color palette, medium shot.",
  "negative_prompt": "photorealistic, 3D render, CGI, photo, photograph, realistic proportions, overly detailed textures, modern anime style, harsh lines, dark, gloomy, watermark, text, signature, blurry, unfocused",
  "style_raw": "Studio Ghibli anime style",
  "aspect_ratio": "16:9"
}</code></pre>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Original</th>
<th>Edited Version</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img src="6.png" width="80%"></td>
<td><img src="11.png" width="80%"></td>
</tr>
</tbody>
</table>
<p>This was <strong>successful</strong>! <br> Gemini was able to edit photos correctly. <br></p>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Although Gemini 2.0 Flash Experimental didn’t create the most realistic photos, nor did it do the best with text in an image, it did a create job of editing the style of the photo when I asked it to create a “Studio Ghibli” version. <br> Perhaps there was something wrong with my versions, because it only provided the prompt for the images and I had to enter it into ChatGPT 4o to create the images. <br> I really liked the <strong>thoughts</strong> section of Gemini, it breaks down how it interpretted my prompting and was able to explain most of how it created these images. <br> All in all, I would use Gemini to modify the style of pictures in the future. I wouldn’t use it for text, but it’s great for a more creative use.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/jdeng75\.github\.io\/comm4190_S25_Using_LLMs_Blog\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>