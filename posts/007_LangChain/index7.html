<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.30">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jenn Deng">
<meta name="dcterms.date" content="2024-02-24">
<meta name="description" content="Jenn’s teacher-hat-mode-initiated">

<title>Playing Around With LangChain – Jenn Deng’s Exploration with LLMs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-de070a7b0ab54f8780927367ac907214.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-d73a3becde7ab5743b8a88b26cbb919d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Jenn Deng’s Exploration with LLMs</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Playing Around With LangChain</h1>
                  <div>
        <div class="description">
          Jenn’s teacher-hat-mode-initiated
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">LLMs</div>
                <div class="quarto-category">prompting</div>
                <div class="quarto-category">langchain</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Jenn Deng </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 24, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="what-is-the-term-large-language-model" class="level1">
<h1>What is the term <strong>large language model</strong></h1>
<p><img src="1.png" width="100%"></p>
<p><strong>Large Language Models:</strong> <br> - <em>Large</em> describes the size of these models based on the training data and parameters.<br> - <em>Language model</em> describes the computer algorithm trained to receive written text and to create an output.<br> - Large language models are trained on vast amounts of text and have learned from the patterns in large data sets.<br></p>
<p><strong>Fun Fact</strong> : Most models are better at English than they are in other languages because of the prevalence of English in training models.</p>
</section>
<section id="how-does-langchain-come-into-play" class="level1">
<h1>How Does LangChain Come Into Play</h1>
<p><img src="2.png" width="100%"></p>
<p>On October 22, 2022, Harrison Chase was the first the publish the first commit on GitHub for the LangChain open source library. <br> LangChain was created after discovering that LLM applications needed to use LLMs together with “other sources of computation or knowledge”. <br> It creates simple abstractions for each major prompting technique.</p>
</section>
<section id="getting-set-up-with-langchain" class="level1">
<h1>Getting Set Up With LangChain</h1>
<p>I’ll be referencing O’Reilly “Chapter 1.LLM Fundementals With LangChain” if you want to follow along! We’re going to learn how to work with LangChain’s building blocks to understand more about LLM concepts. <br> LangChain has two interfaces to interact with any LLM API provider<br> - Chat models<br> - LLMS</p>
<section id="example-1-im-at-a-capital-one.." class="level5">
<h5 class="anchored" data-anchor-id="example-1-im-at-a-capital-one.."><strong>Example 1: I’m at a Capital One..”</strong></h5>
<details>
<summary>
Code
</summary>
<p>from langchain_openai.chat_models import ChatOpenAI</p>
<p>model = ChatOpenAI(model=“gpt-3.5-turbo”)</p>
response = model.invoke(“I’m at a capital one”) print(response)
</details>
</section>
<section id="response-1" class="level5">
<h5 class="anchored" data-anchor-id="response-1">Response 1</h5>
<details>
<summary>
Output
</summary>
content=‘bank branch. How can I assist you today?’ additional_kwargs={‘refusal’: None} response_metadata={‘token_usage’: {‘completion_tokens’: 11, ‘prompt_tokens’: 13, ‘total_tokens’: 24, ‘completion_tokens_details’: {‘accepted_prediction_tokens’: 0, ‘audio_tokens’: 0, ‘reasoning_tokens’: 0, ‘rejected_prediction_tokens’: 0}, ‘prompt_tokens_details’: {‘audio_tokens’: 0, ‘cached_tokens’: 0}}, ‘model_name’: ‘gpt-3.5-turbo-0125’, ‘system_fingerprint’: None, ‘finish_reason’: ‘stop’, ‘logprobs’: None} id=‘run-80a164c8-60dd-447f-a128-6e6482c0df04-0’ usage_metadata={‘input_tokens’: 13, ‘output_tokens’: 11, ‘total_tokens’: 24, ‘input_token_details’: {‘audio’: 0, ‘cache_read’: 0}, ‘output_token_details’: {‘audio’: 0, ‘reasoning’: 0}}
</details>
<p>Let’s break this response down. <br> 1. <strong>The Main Response is (content)</strong> <br></p>
<ul>
<li>“bank branch. How can I assist you today”</li>
<li>This is the actual AI-generated response to the input <em>“I’m at a capital one”</em> –&gt; <em>AI responds: “bank branch. How can I assist you today?”</em></li>
<li>The model recognized Capital One as a <strong>bank</strong> and responded.</li>
</ul>
<ol start="2" type="1">
<li><strong>Additional Information</strong> <br></li>
</ol>
<ul>
<li>additional_kwargs={‘refusal’: None}</li>
<li>This is an extra metadata field and it suggests that the model did not reguse to answer the question</li>
</ul>
<ol start="3" type="1">
<li><strong>Token Ussage</strong></li>
</ol>
<ul>
<li>This gives token usage stats</li>
<li><strong>prompt_tokens:13</strong> is the input that used 13 tokens</li>
<li><strong>completion_tokens:11</strong> is the model’s response using 11 tokens</li>
<li><strong>total_tokens:24</strong> is the total tokens used for the request</li>
</ul>
<ol start="4" type="1">
<li><strong>Model information</strong></li>
</ol>
<ul>
<li>The request was processed using <strong>GPT-3.5-turbo-0125</strong>, a specific version of OpenAI’s GPT-3.5.</li>
</ul>
</section>
<section id="was-the-response-correct-to-what-i-was-trying-to-say" class="level5">
<h5 class="anchored" data-anchor-id="was-the-response-correct-to-what-i-was-trying-to-say"><strong>Was the response correct to what I was trying to say</strong> <br></h5>
<p>Um… actually not. I was trying to tell the audience “I’m at a Capital One cafe drinking a caramel latte and completing my blogs” but I understand why it generated that message. Capital one is a bank, and there are assistants that ask if we need help with any services. I rate it a 7/10 response!</p>
</section>
<section id="other-useful-parameters" class="level5">
<h5 class="anchored" data-anchor-id="other-useful-parameters">Other Useful Parameters</h5>
<ul>
<li><em>temperature</em> <br> Controls the sampling algorithm for the output. The lower the value, the more predictable outputs. Higher values generate more creative results.</li>
<li><em>max_tokens</em> <br> This limits the size and cost of the output. A lower value causes the LLM to stop generting properly..</li>
</ul>
</section>
</section>
<section id="system-role-vs-user-role-vs-assistant-role" class="level1">
<h1>System Role vs User Role vs Assistant Role</h1>
<ul>
<li><strong>System Role</strong> <br> Used for instructions the model should use to answer</li>
<li><strong>User Role</strong> <br> Used for the user’s query and other stuff created by the user</li>
<li><strong>Assistant Role</strong> <br> Used for content generated by the model</li>
</ul>
<p>Chat models have different types of chat messaging interfaces with the roles listed above and here are some of the following:</p>
<ul>
<li><em>HumanMessage (User Role)</em> <br> A message sent from the perspective of a human</li>
<li><em>AIMessage (Assistant Role)</em> <br> A message sent from the perspective of the AI that the human is interacting with</li>
<li><em>SystemMessage (System Role)</em> <br> A message setting the instructions that AI should follow</li>
<li><em>ChatMessage</em> <br> A message allowing for arbitrary setting of role</li>
</ul>
<section id="same-messages-different-fonts" class="level5">
<h5 class="anchored" data-anchor-id="same-messages-different-fonts"><strong>Same Messages, Different Fonts</strong></h5>
</section>
<section id="humanmessage" class="level5">
<h5 class="anchored" data-anchor-id="humanmessage">HumanMessage</h5>
<details>
<summary>
Code
</summary>
<p>from langchain_openai.chat_models import ChatOpenAI from langchain_core.messages import HumanMessage</p>
<p>model = ChatOpenAI() prompt = [HumanMessage(“I’m not completely sure what I should do for my spring break trip, what should I do and who should I go with?”)]</p>
model.invoke(prompt)
</details>
<details>
<summary>
Output
</summary>
“There are many options for spring break trips depending on your interests and budget. Here are a few suggestions: <br> Beach vacation: If you enjoy relaxing on the beach and soaking up the sun, consider booking a trip to a tropical destination like Hawaii, Mexico, or the Caribbean. You can go with friends, family, or even solo. <br> Adventure trip: If you’re looking for a more active vacation, consider going on a hiking or camping trip. You could explore national parks like Yellowstone or Yosemite, or go on a backpacking trip in a new country.<br> City getaway: If you prefer exploring cities and trying new foods and experiences, consider booking a trip to a vibrant city like New York, Chicago, or San Francisco. You could go with a group of friends or your significant other.<br> Ultimately, the best trip for you will depend on your interests and budget. Consider talking to friends and family members to get their recommendations, and don’t be afraid to try something new and exciting. Just remember to plan ahead and make sure you have everything you need for a safe and enjoyable trip.”
</details>
<details>
<summary>
Additional Info
</summary>
dditional_kwargs={‘refusal’: None}, response_metadata={‘token_usage’: {‘completion_tokens’: 222, ‘prompt_tokens’: 33, ‘total_tokens’: 255, ‘completion_tokens_details’: {‘accepted_prediction_tokens’: 0, ‘audio_tokens’: 0, ‘reasoning_tokens’: 0, ‘rejected_prediction_tokens’: 0}, ‘prompt_tokens_details’: {‘audio_tokens’: 0, ‘cached_tokens’: 0}}, ‘model_name’: ‘gpt-3.5-turbo-0125’, ‘system_fingerprint’: None, ‘finish_reason’: ‘stop’, ‘logprobs’: None}, id=‘run-ca796ee2-d279-4e51-9c93-fce7fba98470-0’, usage_metadata={‘input_tokens’: 33, ‘output_tokens’: 222, ‘total_tokens’: 255, ‘input_token_details’: {‘audio’: 0, ‘cache_read’: 0}, ‘output_token_details’: {‘audio’: 0, ‘reasoning’: 0}})
</details>
</section>
<section id="systemmessage" class="level5">
<h5 class="anchored" data-anchor-id="systemmessage">SystemMessage</h5>
<details>
<summary>
Code
</summary>
<p>from langchain_core.messages import HumanMessage, SystemMessage from langchain_openai.chat_models import ChatOpenAI</p>
<p>model = ChatOpenAI() system_msg = SystemMessage( ’‘’You are a travel assistant who gets a big commission if you help clients book trips that they love.’’’ ) human_msg = HumanMessage(‘I am not completely sure what I should do for my spring break trip, what should I do and who should I go with?’)</p>
model.invoke([system_msg, human_msg])
</details>
<details>
<summary>
Output
</summary>
“Planning a trip for spring break sounds like a great idea! To determine what you should do and who you should go with, it’s important to consider your interests, budget, and preferred travel style. Here are a few questions to help guide your decision-making process:<br> 1. What type of experience are you looking for? Are you seeking relaxation on a beach, an adventure-filled trip, cultural exploration, or something else entirely?<br> What is your budget for the trip? This will help narrow down destinations and activities within your financial means.<br> Do you prefer traveling solo, with friends, with family, or with a partner? Consider who you would enjoy spending time with and who shares similar interests.<br> Are there any specific destinations you’ve always wanted to visit or activities you’ve been wanting to try? Spring break is a great opportunity to tick off items on your bucket list.<br> Once you’ve thought about these questions, feel free to share some more details about your preferences, and I can help suggest specific destinations and activities that align with your interests. Let’s make sure your spring break trip is one to remember!”
</details>
<details>
<summary>
Additional Info
</summary>
, additional_kwargs={‘refusal’: None}, response_metadata={‘token_usage’: {‘completion_tokens’: 226, ‘prompt_tokens’: 57, ‘total_tokens’: 283, ‘completion_tokens_details’: {‘accepted_prediction_tokens’: 0, ‘audio_tokens’: 0, ‘reasoning_tokens’: 0, ‘rejected_prediction_tokens’: 0}, ‘prompt_tokens_details’: {‘audio_tokens’: 0, ‘cached_tokens’: 0}}, ‘model_name’: ‘gpt-3.5-turbo-0125’, ‘system_fingerprint’: None, ‘finish_reason’: ‘stop’, ‘logprobs’: None}, id=‘run-d4d25e15-68a9-490b-b7e5-31b3365322e6-0’, usage_metadata={‘input_tokens’: 57, ‘output_tokens’: 226, ‘total_tokens’: 283, ‘input_token_details’: {‘audio’: 0, ‘cache_read’: 0}, ‘output_token_details’: {‘audio’: 0, ‘reasoning’: 0}})
</details>
</section>
<section id="analysis-of-responses" class="level5">
<h5 class="anchored" data-anchor-id="analysis-of-responses"><strong>Analysis of Responses</strong> <br></h5>
<p>The <em>HumanMessage</em> sounds very suggestive and informal. It offers a range of places to go and makes good recommendations, like a friend would <br> However, I do prefer <em>SystemMessage</em>. This is no surprise as I gave it the model of being a really good travel agent. It asks the correct questions to dig into what type of trip I want. And the exclamation marks makes the model sound very friendly <br> The total tokens used for the HumanMessage was 255, while the total tokens used for the SystemMessage was 283. More tokens were used for the SystemMessage, and I’m curious to see if it’s because I set a specif model or if the response was more creative. More food for thought!</p>
</section>
</section>
<section id="making-llm-prompts-reusable" class="level1">
<h1>Making LLM Prompts Reusable</h1>
<p>We just did a lot of prompting, but now let’s try to de more of a detailed prompt:</p>
<ul>
<li>Answer the question based on the context listed below. If the question cannot be answered with the information given, answer with “Sorry I do not know”. <br></li>
</ul>
<p><strong>Context</strong>: There are many different types of coffees, but I particularly love very strong coffee taste. <br></p>
<p><strong>Question</strong>: What coffee should I try</p>
<p>It looks simple… but it is a bit hard to figure out what the text should contain and how it should create the response based on the user’s input. <br> Lucikly, <strong>LangChain</strong> provides prompt template interfaces to create answers with dynamic inputs:</p>
<details>
<summary>
Code
</summary>
<p>from langchain_core.prompts import PromptTemplate</p>
<p>template = PromptTemplate.from_template(“““Answer the question based on the context listed below. If the question cannot be answered with the information given, answer with”Sorry I do not know”.</p>
<p>Context: {context}</p>
<p>Question: {question}</p>
<p>Answer: “““)</p>
template.invoke({ “context”: “““There are many different types of coffees, but I particularly love very strong coffee taste.”““,”question”: “What coffee should I try?” })
</details>
<p>The template can be used as a base for building multiple static, specific prompts. When you format the prompt with specific values, context and question, you get a static prompt ready to be passed into an LLM. <br> <strong>Let’s feed it into an LLM OpenAI model using LangChain</strong>:</p>
<div id="b8de30ff-ea47-4642-9581-8782a087925a" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain_openai <span class="im">import</span> OpenAI</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain_core.prompts <span class="im">import</span> PromptTemplate</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.chains <span class="im">import</span> LLMChain  <span class="co"># This is necessary for linking the prompt and model</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dotenv <span class="im">import</span> load_dotenv</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load API key</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> load_dotenv()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the prompt template</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>template <span class="op">=</span> PromptTemplate.from_template(<span class="st">"""Answer the question based on the context</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="st">listed below. If the question cannot be answered with the information </span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="st">given, answer with "Sorry I do not know".</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="st">Context: </span><span class="sc">{context}</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="st">Question: </span><span class="sc">{question}</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="st">Answer:"""</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the model</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> OpenAI()</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a chain that uses the prompt and the model</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>chain <span class="op">=</span> LLMChain(prompt<span class="op">=</span>template, llm<span class="op">=</span>llm)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Now invoke the chain with your input</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> chain.invoke({</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">"context"</span>: <span class="st">"There are many different types of coffees, but I particularly love very strong coffee taste."</span>,</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">"question"</span>: <span class="st">"What coffee should I try?"</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response[<span class="st">'text'</span>])  <span class="co"># This prints the generated answer</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> Sorry I do not know.</code></pre>
</div>
</div>
</section>
<section id="analysis" class="level1">
<h1>Analysis</h1>
<p><br></p>
<p>The LangChain chain is responding with “Sorry I do not know” because LangChain + Open AI models follows instructions closely. If my context doesn’t contain a specific, answerable detail related to the question, the model will default to my fallback instruction. <br></p>
<p>There are ways to fix this or test this.</p>
<ol type="1">
<li>I could make sure the context actually answers the question</li>
<li>Soften or remove the fallback instructions in the prompt.</li>
</ol>
<p><br></p>
<p>I’ll remove the fall back to see what happens.</p>
<div id="02c7378a-e7c8-4359-bff8-7ac7ac48a43d" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain_openai <span class="im">import</span> OpenAI</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain_core.prompts <span class="im">import</span> PromptTemplate</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.chains <span class="im">import</span> LLMChain  <span class="co"># This is necessary for linking the prompt and model</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dotenv <span class="im">import</span> load_dotenv</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load API key</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> load_dotenv()</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the prompt template</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>template <span class="op">=</span> PromptTemplate.from_template(<span class="st">"""Answer the question based on the context</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="st">listed below. ".</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="st">Context: </span><span class="sc">{context}</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="st">Question: </span><span class="sc">{question}</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="st">Answer:"""</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the model</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> OpenAI()</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a chain that uses the prompt and the model</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>chain <span class="op">=</span> LLMChain(prompt<span class="op">=</span>template, llm<span class="op">=</span>llm)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Now invoke the chain with your input</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> chain.invoke({</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">"context"</span>: <span class="st">"There are many different types of coffees, but I particularly love very strong coffee taste."</span>,</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">"question"</span>: <span class="st">"What coffee should I try?"</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response[<span class="st">'text'</span>])  <span class="co"># This prints the generated answer</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> You should try an espresso or a dark roast coffee for a strong coffee taste.</code></pre>
</div>
</div>
</section>
<section id="analysis-1" class="level1">
<h1>Analysis</h1>
<p><br></p>
<p>There it goes! It said I should try an espresso or a dark roast coffee for a strong coffee taste. Without the fall back parameter, I think this could be a super useful tool if you can prompt it correctly. <br></p>
<p>In this post, I explored how LangChain enables deeper interactions with large language models by combining prompts, context, and logic into structured pipelines. LangChain made this possible by letting me structure the interaction with a prompt template and link it to a language model (OpenAI), creating a system that could answer questions based on specific data and even self-limit when information was missing. This balance between control and creativity is what makes LangChain so powerful—and worth exploring for anyone working with AI-driven workflows.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/jdeng75\.github\.io\/comm4190_S25_Using_LLMs_Blog\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>