{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cdaa2f12-824d-45fa-ba5e-95efb338268e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Playing Around With LangChain\"\n",
    "description: \"Jenn's teacher-hat-mode-initiated\"\n",
    "author: \"Jenn Deng\"\n",
    "date: \"2/24/2024\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - prompting\n",
    "  - langchain\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf3ddc-9630-40d2-86e2-78ed06349822",
   "metadata": {},
   "source": [
    "# What is the term **large language model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f98317c-ebc5-4b81-9948-912694f78e5f",
   "metadata": {},
   "source": [
    "<img src=\"1.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf06ed-b81d-4600-bd9a-580e857047ad",
   "metadata": {},
   "source": [
    "**Large Language Models:** </br>\n",
    "- *Large* describes the size of these models based on the training data and parameters.</br>\n",
    "- *Language model* describes the computer algorithm trained to receive written text and to create an output.</br>\n",
    "- Large language models are trained on vast amounts of text and have learned from the patterns in large data sets.</br>\n",
    "\n",
    "**Fun Fact** : Most models are better at English than they are in other languages because of the prevalence of English in training models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21681d20-4ba2-4768-9622-b10a49a8f81f",
   "metadata": {},
   "source": [
    "# How Does LangChain Come Into Play"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d2bb4a-1a79-4f98-bda8-e7e69612a543",
   "metadata": {},
   "source": [
    "<img src=\"2.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c2c8c6-474f-4929-9efe-ccc7b081b0fa",
   "metadata": {},
   "source": [
    "On October 22, 2022, Harrison Chase was the first the publish the first commit on GitHub for the LangChain open source library. </br>\n",
    "LangChain was created after  discovering that LLM applications needed to use LLMs together with \"other sources of computation or knowledge\". </br>\n",
    "It creates simple abstractions for each major prompting technique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5aa6c2-ccb6-450e-b52f-677d7a0a9b43",
   "metadata": {},
   "source": [
    "# Getting Set Up With LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5c018c-bffe-4f3b-957e-916777c59823",
   "metadata": {},
   "source": [
    "I'll be referencing O'Reilly \"Chapter 1.LLM Fundementals With LangChain\" if you want to follow along! We're going to learn how to work with LangChain's building blocks to understand more about LLM concepts. </br>\n",
    "LangChain has two interfaces to interact with any LLM API provider</br>\n",
    "- Chat models</br>\n",
    "- LLMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a484d7-715d-4898-9ea8-cc050c8b1d8b",
   "metadata": {},
   "source": [
    "##### **Example 1: I'm at a Capital One..\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0a6c18-1f70-42e8-afcc-339f99c2e79f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Code</summary>\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "response = model.invoke(\"I'm at a capital one\")\n",
    "print(response)\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae4a97-8c65-48cf-ab60-d6e95c7b08c9",
   "metadata": {},
   "source": [
    "##### Response 1\n",
    "\n",
    "<details>\n",
    "<summary>Output</summary>\n",
    "content='bank branch. How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 13, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-80a164c8-60dd-447f-a128-6e6482c0df04-0' usage_metadata={'input_tokens': 13, 'output_tokens': 11, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d7a44-3270-4371-b424-9ddb71a6168b",
   "metadata": {},
   "source": [
    "Let's break this response down. </br>\n",
    "1. **The Main Response is (content)** </br>\n",
    "   \n",
    "- \"bank branch. How can I assist you today\"\n",
    "- This is the actual AI-generated response to the input *\"I'm at a capital one\"* --> *AI responds: \"bank branch. How can I assist you today?\"*\n",
    "- The model recognized Capital One as a **bank** and responded.\n",
    "  \n",
    "2. **Additional Information** </br>\n",
    "   \n",
    "- additional_kwargs={'refusal': None}\n",
    "- This is an extra metadata field and it suggests that the model did not reguse to answer the question\n",
    "  \n",
    "3. **Token Ussage**\n",
    "- This gives token usage stats\n",
    "- **prompt_tokens:13** is the input that used 13 tokens\n",
    "- **completion_tokens:11** is the model's response using 11 tokens\n",
    "- **total_tokens:24** is the total tokens used for the request\n",
    "  \n",
    "4. **Model information**\n",
    "- The request was processed using **GPT-3.5-turbo-0125**, a specific version of OpenAI's GPT-3.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d4a7e1-1c49-4983-8a6a-5e956cc390b6",
   "metadata": {},
   "source": [
    "##### **Was the response correct to what I was trying to say** </br>\n",
    "Um... actually not. I was trying to tell the audience \"I'm at a Capital One cafe drinking a caramel latte and completing my blogs\" but I understand why it generated that message. Capital one is a bank, and there are assistants that ask if we need help with any services. I rate it a 7/10 response!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76b67b-1d52-49c6-8e6f-0fc2ae1f9125",
   "metadata": {},
   "source": [
    "##### Other Useful Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54247d41-8d37-4161-93ee-43897e8cd62d",
   "metadata": {},
   "source": [
    "- *temperature* </br>\n",
    "Controls the sampling algorithm for the output. The lower the value, the more predictable outputs. Higher values generate more creative results.\n",
    "- *max_tokens* </br>\n",
    "This limits the size and cost of the output. A lower value causes the LLM to stop generting properly.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664a23ab-8246-43df-8e2f-e603136eb5a9",
   "metadata": {},
   "source": [
    "# System Role vs User Role vs Assistant Role "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d45b7-3bf7-458f-9b8f-1472e5559540",
   "metadata": {},
   "source": [
    "- **System Role** </br>\n",
    "Used for instructions the model should use to answer\n",
    "- **User Role** </br>\n",
    "Used for the user's query and other stuff created by the user\n",
    "- **Assistant Role** </br>\n",
    "Used for content generated by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c6c4c-0918-48f9-8e25-e407048f3b7e",
   "metadata": {},
   "source": [
    "Chat models have different types of chat messaging interfaces with the roles listed above and here are some of the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2700a-eff0-43cb-84ee-8e1aeb181b3a",
   "metadata": {},
   "source": [
    "- *HumanMessage (User Role)* </br>\n",
    "A message sent from the perspective of a human \n",
    "- *AIMessage (Assistant Role)* </br>\n",
    "A message sent from the perspective of the AI that the human is interacting with\n",
    "- *SystemMessage (System Role)* </br>\n",
    "A message setting the instructions that AI should follow\n",
    "- *ChatMessage* </br>\n",
    "A message allowing for arbitrary setting of role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a552d5-49a2-4069-8a64-0c53861759c8",
   "metadata": {},
   "source": [
    "##### **Same Messages, Different Fonts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc4c85-30ca-4747-99fd-ccde5e3cfd57",
   "metadata": {},
   "source": [
    "##### HumanMessage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d51aa-3fbc-481f-ba12-9327135baf1f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Code</summary>\n",
    "    \n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = [HumanMessage(\"I'm not completely sure what I should do for my spring break trip, what should I do and who should I go with?\")]\n",
    "\n",
    "model.invoke(prompt)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e3730-7179-4434-9b71-0a6de2288912",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Output</summary>\n",
    "\"There are many options for spring break trips depending on your interests and budget. Here are a few suggestions: </br>\n",
    "Beach vacation: If you enjoy relaxing on the beach and soaking up the sun, consider booking a trip to a tropical destination like Hawaii, Mexico, or the Caribbean. You can go with friends, family, or even solo. </br>\n",
    "Adventure trip: If you're looking for a more active vacation, consider going on a hiking or camping trip. You could explore national parks like Yellowstone or Yosemite, or go on a backpacking trip in a new country.</br>\n",
    "City getaway: If you prefer exploring cities and trying new foods and experiences, consider booking a trip to a vibrant city like New York, Chicago, or San Francisco. You could go with a group of friends or your significant other.</br>\n",
    "Ultimately, the best trip for you will depend on your interests and budget. Consider talking to friends and family members to get their recommendations, and don't be afraid to try something new and exciting. Just remember to plan ahead and make sure you have everything you need for a safe and enjoyable trip.\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec32e39-9e12-4a07-98dc-934673b8c5d0",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Additional Info</summary>\n",
    "dditional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 222, 'prompt_tokens': 33, 'total_tokens': 255, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ca796ee2-d279-4e51-9c93-fce7fba98470-0', usage_metadata={'input_tokens': 33, 'output_tokens': 222, 'total_tokens': 255, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed224378-7ce5-468a-b126-4b0a06b178d5",
   "metadata": {},
   "source": [
    "##### SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473b5be-8f0f-4374-97e3-435174e7e192",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Code</summary>\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "system_msg = SystemMessage(\n",
    "    '''You are a travel assistant who gets a big commission if you help clients book trips that they love.'''\n",
    ")\n",
    "human_msg = HumanMessage('I am not completely sure what I should do for my spring break trip, what should I do and who should I go with?')\n",
    "\n",
    "model.invoke([system_msg, human_msg])\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453c725-52f2-4e1c-b692-423638da7189",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Output</summary>\n",
    "\"Planning a trip for spring break sounds like a great idea! To determine what you should do and who you should go with, it's important to consider your interests, budget, and preferred travel style. Here are a few questions to help guide your decision-making process:</br>\n",
    "1. What type of experience are you looking for? Are you seeking relaxation on a beach, an adventure-filled trip, cultural exploration, or something else entirely?</br>\n",
    "What is your budget for the trip? This will help narrow down destinations and activities within your financial means.</br>\n",
    "Do you prefer traveling solo, with friends, with family, or with a partner? Consider who you would enjoy spending time with and who shares similar interests.</br>\n",
    "Are there any specific destinations you've always wanted to visit or activities you've been wanting to try? Spring break is a great opportunity to tick off items on your bucket list.</br>\n",
    "Once you've thought about these questions, feel free to share some more details about your preferences, and I can help suggest specific destinations and activities that align with your interests. Let's make sure your spring break trip is one to remember!\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e95e5-0c8b-4ae2-80d5-9efe4f01ede2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Additional Info</summary>\n",
    ", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 226, 'prompt_tokens': 57, 'total_tokens': 283, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d4d25e15-68a9-490b-b7e5-31b3365322e6-0', usage_metadata={'input_tokens': 57, 'output_tokens': 226, 'total_tokens': 283, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef69ff49-3fc2-4462-91e9-4cc76e26d4c3",
   "metadata": {},
   "source": [
    "##### **Analysis of Responses** </br>\n",
    "The *HumanMessage* sounds very suggestive and informal. It offers a range of places to go and makes good recommendations, like a friend would </br>\n",
    "However, I do prefer *SystemMessage*. This is no surprise as I gave it the model of being a really good travel agent. It asks the correct questions to dig into what type of trip I want. And the exclamation marks makes the model sound very friendly </br>\n",
    "The total tokens used for the HumanMessage was 255, while the total tokens used for the SystemMessage was 283. More tokens were used for the SystemMessage, and I'm curious to see if it's because I set a specif model or if the response was more creative. More food for thought!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df653864-768e-4b8e-ac66-6213f34e4bca",
   "metadata": {},
   "source": [
    "# Making LLM Prompts Reusable \n",
    "We just did a lot of prompting, but now let's try to de more of a detailed prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1b0715-4cb3-4b7b-a233-c944d7142ea7",
   "metadata": {},
   "source": [
    "* Answer the question based on the context listed below. If the question cannot be answered with the information given, answer with \"Sorry I do not know\". </br>\n",
    "\n",
    "**Context**: There are many different types of coffees, but I particularly love very strong coffee taste. </br>\n",
    "\n",
    "**Question**: What coffee should I try"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c966e99e-6f3f-443a-a522-45e295a1f2a0",
   "metadata": {},
   "source": [
    "It looks simple... but it is a bit hard to figure out what the text should contain and how it should create the response based on the user's input. </br>\n",
    "Lucikly, **LangChain** provides prompt template interfaces to create answers with dynamic inputs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5acce2-9034-4255-8905-13d1fcae7405",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Code</summary>\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context\n",
    "listed below. If the question cannot be answered with the information given, answer with \"Sorry I do not know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "template.invoke({\n",
    "    \"context\": \"\"\"There are many different types of \n",
    "    coffees, but I particularly love very strong coffee taste.\"\"\",\n",
    "    \"question\": \"What coffee should I try?\"\n",
    "})\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae69f267-c29c-4699-961a-a962840738f9",
   "metadata": {},
   "source": [
    "The template can be used as a base for building multiple static, specific prompts. When you format the prompt with specific values, context and question, you get a static prompt ready to be passed into an LLM. </br>\n",
    "**Let's feed it into an LLM OpenAI model using LangChain**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8de30ff-ea47-4642-9581-8782a087925a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sorry I do not know.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain  # This is necessary for linking the prompt and model\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load API key\n",
    "_ = load_dotenv()\n",
    "\n",
    "# Define the prompt template\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context\n",
    "listed below. If the question cannot be answered with the information \n",
    "given, answer with \"Sorry I do not know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\")\n",
    "\n",
    "# Initialize the model\n",
    "llm = OpenAI()\n",
    "\n",
    "# Create a chain that uses the prompt and the model\n",
    "chain = LLMChain(prompt=template, llm=llm)\n",
    "\n",
    "# Now invoke the chain with your input\n",
    "response = chain.invoke({\n",
    "    \"context\": \"There are many different types of coffees, but I particularly love very strong coffee taste.\",\n",
    "    \"question\": \"What coffee should I try?\"\n",
    "})\n",
    "\n",
    "print(response['text'])  # This prints the generated answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6403e2a9-59fa-4260-bd28-ea68e73688fa",
   "metadata": {},
   "source": [
    "# Analysis \n",
    "</br>\n",
    "\n",
    "The LangChain chain is responding with \"Sorry I do not know\" because LangChain + Open AI models follows instructions closely. If my context doesn't contain a specific, answerable detail related to the question, the model will default to my fallback instruction. \n",
    "</br>\n",
    "\n",
    "There are ways to fix this or test this.\n",
    "\n",
    "1. I could make sure the context actually answers the question\n",
    "2. Soften or remove the fallback instructions in the prompt.\n",
    "\n",
    "</br>\n",
    "\n",
    "I'll remove the fall back to see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02c7378a-e7c8-4359-bff8-7ac7ac48a43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You should try an espresso or a dark roast coffee for a strong coffee taste.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain  # This is necessary for linking the prompt and model\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load API key\n",
    "_ = load_dotenv()\n",
    "\n",
    "# Define the prompt template\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context\n",
    "listed below. \".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\")\n",
    "\n",
    "# Initialize the model\n",
    "llm = OpenAI()\n",
    "\n",
    "# Create a chain that uses the prompt and the model\n",
    "chain = LLMChain(prompt=template, llm=llm)\n",
    "\n",
    "# Now invoke the chain with your input\n",
    "response = chain.invoke({\n",
    "    \"context\": \"There are many different types of coffees, but I particularly love very strong coffee taste.\",\n",
    "    \"question\": \"What coffee should I try?\"\n",
    "})\n",
    "\n",
    "print(response['text'])  # This prints the generated answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d344912-80e2-46f9-9b6d-4f1b275ccf5b",
   "metadata": {},
   "source": [
    "# Analysis \n",
    "</br>\n",
    "\n",
    "There it goes! It said I should try an espresso or a dark roast coffee for a strong coffee taste. Without the fall back parameter, I think this could be a super useful tool if you can prompt it correctly.\n",
    "</br>\n",
    "\n",
    "In this post, I explored how LangChain enables deeper interactions with large language models by combining prompts, context, and logic into structured pipelines. \n",
    "LangChain made this possible by letting me structure the interaction with a prompt template and link it to a language model (OpenAI), creating a system that could answer questions based on specific data and even self-limit when information was missing. This balance between control and creativity is what makes LangChain so powerfulâ€”and worth exploring for anyone working with AI-driven workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
