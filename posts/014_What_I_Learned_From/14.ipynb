{
 "cells": [
  {
   "cell_type": "raw",
   "id": "775c24c8-e850-4624-adcc-8a6fcd5893b9",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"What I learned from 'Watch an A.I. Learn to Write by Reading Nothing but ___'\"\n",
    "description: \"BabyGPT! nanoGPT!\"\n",
    "author: \"Jenn Deng\"\n",
    "date: \"3/27/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - prompting\n",
    "  - logic\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b961a9d-f88c-4bed-ae96-ecce881feb1d",
   "metadata": {},
   "source": [
    "<img src=\"11.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf67612-428c-4ce6-a7d7-a9c772edff57",
   "metadata": {},
   "source": [
    "# Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e321b1cf-adb9-40db-a0d0-536cffe3c927",
   "metadata": {},
   "source": [
    "In The New York Times article \"Watch an AI Learn to Write By Reading Nothing But ___\" by **Aatish Bhatia**, he spends weeks reading AI research articles and training tiny language models on his computer. In his aticle, he uses \"BabyGPT\" to prompt AI to learn language by reading only the complete works of the readers' choosing; Jane Austin, Shakespeare, Federalist Papers, Moby-Dick, Star Trek: The Next Generation, and Harry Potter. This BabyAI only sees 900 thousand words and nothing else. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ce8c74-0475-4944-bee5-f8d9c8cce110",
   "metadata": {},
   "source": [
    "Before training, there's gibberish. You can't read it or understand what the BabyGPT is trying to produce. When you click on \"Generate another response\" it ends up being another pile of gibberish. You can tell that BabyGPT is ant-size in comparison to using other models. This example shows how language models start off- there's clumps of randomly produced combinations of characters. These models need to train after many rounds to eventually generate responses that we can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea15099d-4d67-47f5-a000-a326352c454a",
   "metadata": {},
   "source": [
    "<img src=\"1.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b7889-c964-4ad6-8359-b0eb94d3adf2",
   "metadata": {},
   "source": [
    "In the article, after **250** rounds, we get some English letters. They noticed that there's a lot of letter \"e\" because that's the most common letter in English. Also, there are some small words that form, such as I, to, you, etc. It's still not great, but it's something readable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3445bb1-532f-4aa2-918d-55967a1f2e85",
   "metadata": {},
   "source": [
    "After **500** rounds, small words start forming. From the Shakespeare example, I see words such as \"lover\", \"him\", \"they\", etc. There's small and some basic grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eef2f9e-579e-4992-ace6-14407c1a5d8a",
   "metadata": {},
   "source": [
    "And after **5,000** rounds, there are even bigger words now. You see words like \"charges\", \"concludes\", and others. The sentences still don't cohesively make sense, but you can see the progress. The grammar is getting better too. We get this because BabyGPT is a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3f86e-66c7-4bd1-a7ec-c714d9426b52",
   "metadata": {},
   "source": [
    "Finally, after **30,000** rounds, we get full sentences. The words still don't really make sense, but it's looking more like English."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86084b5f-cfb0-4498-bce2-6d1a6cc66845",
   "metadata": {},
   "source": [
    "**Let's explore this for ourselves** </br>\n",
    "</br>\n",
    "**nanoGPT**, developed by **Andrej Karpathy**, is the model used in this article. It's a generative pre-trained transformer. The difference in nanoGPT and chatGPT os the size because GPT-3 was training up to a million times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b6a6b-c5f8-48b6-9331-3c7d4d6f7931",
   "metadata": {},
   "source": [
    "# nanoGPT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e371a-3e88-4bbf-9ef3-8f81f60bc500",
   "metadata": {},
   "source": [
    "I wanted to look to see how they were able to do these training experiments, so I took a look at the original github. Here's just a summary of what I learn, hope it's useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987cc9b1-ba15-4874-ad20-c8915f12fdf2",
   "metadata": {},
   "source": [
    "**Here's the github link if you want to explore nanoGPT** : https://github.com/karpathy/nanoGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7742452-4d03-4966-b70c-bc248b855528",
   "metadata": {},
   "source": [
    "### README"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdde2ac0-3734-44ef-a9a9-980a2ab1e173",
   "metadata": {},
   "source": [
    "karpathy gives an overview of what nanoGPT is and how it prioritizes \"teeth over education.\" </br>\n",
    "</br>\n",
    "The file **train.py** reproduces GPT-2 (124M) on OpenWebText) and has about 4 days of training. The code is a 300-line boilerplate training loop. </br>\n",
    "</br>\n",
    "**Model.py** is a 300-line GPT model definition that loads the GPT-2 weights from OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c40da9-57cc-4b1e-9d6a-b7834384a83e",
   "metadata": {},
   "source": [
    "```\n",
    "pip install torch numpy transformers datasets tiktoken wandb tqdm\n",
    "\n",
    "python data/shakespeare_char/prepare.py\n",
    "python train.py config/train_shakespeare_char.py\n",
    "python sample.py --out_dir=out-shakespeare-char\n",
    "```\n",
    "</br>\n",
    "This generates a few examples: </br>\n",
    "\n",
    "```\n",
    "ANGELO:\n",
    "And cowards it be strawn to my bed,\n",
    "And thrust the gates of my threats,\n",
    "Because he that ale away, and hang'd\n",
    "An one with him.\n",
    "\n",
    "DUKE VINCENTIO:\n",
    "I thank your eyes against it.\n",
    "\n",
    "DUKE VINCENTIO:\n",
    "Then will answer him to save the malm:\n",
    "And what have you tyrannous shall do this?\n",
    "\n",
    "DUKE VINCENTIO:\n",
    "If you have done evils of all disposition\n",
    "To end his power, the day of thrust for a common men\n",
    "That I leave, to fight with over-liking\n",
    "Hasting in a roseman.\n",
    "```\n",
    "This is a character-level model after 3-minutes of training on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd1c37b-6690-48fe-bfdf-502b5a0728ee",
   "metadata": {},
   "source": [
    "### reproducing GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e997d-ddf4-497c-98b7-d0a17d345171",
   "metadata": {},
   "source": [
    "```\n",
    "python data/openwebtext/prepare.py\n",
    "torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e621d2-1a9a-426f-9f3c-c1a059489dde",
   "metadata": {},
   "source": [
    "### baselines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd7b66-87b2-4a4e-b78a-4f51050e7c7c",
   "metadata": {},
   "source": [
    "OpenAI GPT-2 checkpoints allows for baselines in place for openweb text and here are the numbers: </br>\n",
    "</br>\n",
    "```\n",
    "$ python train.py config/eval_gpt2.py\n",
    "$ python train.py config/eval_gpt2_medium.py\n",
    "$ python train.py config/eval_gpt2_large.py\n",
    "$ python train.py config/eval_gpt2_xl.py\n",
    "```\n",
    "</br>\n",
    "Here's the losses on train and val loss. If you don't know what that means: </br>\n",
    "If your train loss is low but val loss is high, that usually means overfitting. If both are high, your model might be underfitting and needs improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47d096-66d0-4151-9307-5fffdfd0ab59",
   "metadata": {},
   "source": [
    "| Loss Type       | What it Measures                   | Used For                         |\n",
    "|------------------|-------------------------------------|----------------------------------|\n",
    "| **Train Loss**    | Error on training data              | Shows how well the model learns |\n",
    "| **Validation Loss** | Error on unseen validation data   | Checks for generalization       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6f2c90-87d7-43aa-a312-d7fcb959a63a",
   "metadata": {},
   "source": [
    "and observe the following losses on train and val: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d6408-355a-4fcc-bd8e-92ccb11d0ce7",
   "metadata": {},
   "source": [
    "| model      | params               | train loss          | val loss |\n",
    "|------------|----------------------|---------------------|----------|\n",
    "| **gpt2**    | 124M                | 3.11                | 3.12     |\n",
    "| **gpt2-medium** | 350M            | 2.85                | 2.84     |\n",
    "| **gpt2-large**  | 774M            | 2.66                | 2.67     |\n",
    "| **gpt2-xl**     | 1558M           | 2.56                | 2.54     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f797769f-fe6a-4644-a62d-eb2fa8a0194c",
   "metadata": {},
   "source": [
    "### finetuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba0930-d7f4-4dca-907c-3f3f11da75e4",
   "metadata": {},
   "source": [
    "Finetuning is very similar to training, it's just training with a smaller learning rate. You make sure to initialize from a pretrained model and train with a smaller learning rate. Here's an example of finetuning that they include: </br>\n",
    "</br>\n",
    "```\n",
    "python train.py config/finetune_shakespeare.py\n",
    "```\n",
    "And when you run the code with: </br>\n",
    "```\n",
    "sample.py --out_dir=out-shakespeare\n",
    "```\n",
    "The output is: </br>\n",
    "```\n",
    "THEODORE:\n",
    "Thou shalt sell me to the highest bidder: if I die,\n",
    "I sell thee to the first; if I go mad,\n",
    "I sell thee to the second; if I\n",
    "lie, I sell thee to the third; if I slay,\n",
    "I sell thee to the fourth: so buy or sell,\n",
    "I tell thee again, thou shalt not sell my\n",
    "possession.\n",
    "\n",
    "JULIET:\n",
    "And if thou steal, thou shalt not sell thyself.\n",
    "\n",
    "THEODORE:\n",
    "I do not steal; I sell the stolen goods.\n",
    "\n",
    "THEODORE:\n",
    "Thou know'st not what thou sell'st; thou, a woman,\n",
    "Thou art ever a victim, a thing of no worth:\n",
    "Thou hast no right, no right, but to be sold.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d50973-80d9-45aa-908e-afec2ef634e8",
   "metadata": {},
   "source": [
    "### sampling / inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486420d4-0402-48da-b8c6-6b4cd4da5aa4",
   "metadata": {},
   "source": [
    "If you want a script to sample from a model you trained for yourself, or sample from a pre-trained GPT-2 model from OpenAI, you can use: </br>\n",
    "```\n",
    "sample.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1383ae2-c308-4571-b197-efea29ddc70c",
   "metadata": {},
   "source": [
    "# Overall Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82594d59-f650-4032-80b5-054899c45ea8",
   "metadata": {},
   "source": [
    "nanoGPT is minimalistic, but is important to help people learn how GPT models work under the hood. It makes it easier for me to understand and study the model's architecture, training, and inference. </br>\n",
    "It's also a pretty modest hardware, like a single GPU. It's different from large-scale models that need massive compute. </br>\n",
    "And unlike the commercial models, nanoGPT offers a transparent, open-source alternative that breaks down how LLMs work. </br>\n",
    "It's a cute little starter kid for GPT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080d442-f6ed-41e2-a0c2-4667063b84a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
