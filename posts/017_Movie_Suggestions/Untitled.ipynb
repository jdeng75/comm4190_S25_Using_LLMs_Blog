{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ebfe8712-ac20-4abb-8bfa-2eae7e8cf436",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Movie Suggestions?: Testing Out Google Prompt Engineering Guide\"\n",
    "description: \"Which prompting technique works best?\"\n",
    "author: \"Jenn Deng\"\n",
    "date: \"05/07/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - prompting\n",
    "  - methods \n",
    "  - movie suggestions\n",
    "  - Google!\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8898e4-ce8a-406e-8ebe-84635029566f",
   "metadata": {},
   "source": [
    "Google claims that \"prompt enginnering can help you be more productive. It also says that using natural language to command a large language model could help propel a lot of the workflow on teams. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f03025-2e70-4887-995d-dc797d8f65b2",
   "metadata": {},
   "source": [
    "Google's **Tips to Becoming a World-Class Prompt Engineeer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043e588b-b464-4182-86d1-7849b45e68b5",
   "metadata": {},
   "source": [
    "<img src=\"1.png\" width=90%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed984bb5-c1da-48a3-a0ac-dbfd56bd8ea7",
   "metadata": {},
   "source": [
    "# Google recently released a 69 page paper on prompt engineering and there are 11 ways it categorizes prompting methods "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc20a3-22c7-4dc1-9d64-402e3cf82f43",
   "metadata": {},
   "source": [
    "1. Direct Prompts (Zero-shot) </br>\n",
    "\n",
    "This type of prompting doesn't provide any additional context or examples, but just provides the model with a direct instruction or question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d55cfd-e250-4ebc-bbfd-2332f8aea0b9",
   "metadata": {},
   "source": [
    "2. One-, few- and multi-shot prompts </br>\n",
    "\n",
    "This method involves providing the model with one or more examples of the desired input-output pars before presenting the actual prompt .This can help the model better understand the task and generate more accurate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca699b-a163-4c8f-99b8-07b546f0fff4",
   "metadata": {},
   "source": [
    "3. Chain of Thought Prompts  </br>\n",
    "\n",
    "CoT prompting encourages the model to break down complex reasoning into a series of intermediate steps, leading to a more comprehensive and wellstructured final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1cfeb-4d9f-4a0c-a125-19a2b7c19611",
   "metadata": {},
   "source": [
    "4. Zero-shot CoT Prompts </br>\n",
    "\n",
    "Combines chain of thought prompting with zero-shot prompting by asking the model to perform reasoning steps, which may often produce better output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8590d9-8190-4f8d-b3ff-046b7f6a928b",
   "metadata": {},
   "source": [
    "5. System prompting \n",
    "</br>\n",
    "\n",
    "Sets the overall context and purpose for the language model. It defines the big picture of what the model should be doing, like translating a language, classifying a review, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e34c704-5a0e-42e9-b4a0-cff65e529a08",
   "metadata": {},
   "source": [
    "6. Contextual prompting \n",
    "</br>\n",
    "\n",
    "Provides specific details or background information relevant to the current conversation or task. It helps the model to understand the nuances of what's being asked and tailor the response accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc8960-73be-4e68-9f28-e6124ef9da60",
   "metadata": {},
   "source": [
    "7. Role Prompting\n",
    "</br>\n",
    "\n",
    "Assigns a specific character or identity for the language model to adopt. This helps the model generate responses that are consistent with the assigned role and its associated knowledge and behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d25e4b-0071-4b5f-9e76-2c061a0aea03",
   "metadata": {},
   "source": [
    "8. Chain of Thought Prompting \n",
    "</br>\n",
    "\n",
    "Chain of Thought prompting is a technique for improving the reasoning capabilities of LLMs by generating immediate reasoning steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5a34c-9484-4eaa-8856-870672744c6e",
   "metadata": {},
   "source": [
    "9. Tree of Thoughts \n",
    "</br>\n",
    "\n",
    "It generalizes the concept of CoT prompting because it allows LLMs to explore multiple different reasoning paths simultaneously, rather than just following a linear chain of thought."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967cc5ca-df8e-4f98-838f-87d3d878c1d2",
   "metadata": {},
   "source": [
    "10. ReAct (reason and act)\n",
    "</br>\n",
    "\n",
    "Is a paradigm for enabling LLMs to solve complex tasks using natural language reasoning combined with external tools allowing the LLM to perform certain actions, such as interacting with external APIs to retrieve information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e5506-38d4-41be-9c2a-d715e14ad212",
   "metadata": {},
   "source": [
    "11. Code prompting \n",
    "</br>\n",
    "\n",
    "Writing prompts for returning code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c53d43-dff8-4c92-9e71-c3c8e3cf7aea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d794375a-8ffd-46d6-a6d0-b38c02e9cead",
   "metadata": {},
   "source": [
    "## Let's try a few of these techniques to see which works the best for me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed85fe-be53-4223-a767-9256a1cd57c1",
   "metadata": {},
   "source": [
    "1. Direct Prompt\n",
    "   \n",
    "</br>\n",
    "\n",
    "<img src=\"2.png\" width=90%/>\n",
    "\n",
    "</br>\n",
    "\n",
    "I like how it doesn't really question me further, but provides random movies across all genres. I like how it asks, \"Want seomthing thoughtful\" or \"In the mood for comedy\" as a way to engage my interests in certain types of movies. However, as a person who's really indecisive, I want to hone in my search. Let's go into other prompting techniques. \n",
    "\n",
    "</br>\n",
    "\n",
    "2. One, few, and multi-shot prompts\n",
    "\n",
    "</br>\n",
    "\n",
    "<img src=\"3.png\" width=90%/>\n",
    "\n",
    "</br>\n",
    "\n",
    "This one got more specific, which helped me because it specified horror movies that are similar to **Hereditary**. For example, the list already has movies that I've watched, and I can confirm that it is very similar to the example movie.\n",
    "\n",
    "</br>\n",
    "\n",
    "3. Chain of Thought Prompting\n",
    "\n",
    "</br>\n",
    "\n",
    "<img src=\"4.png\" width=90%/>\n",
    "\n",
    "</br>\n",
    "\n",
    "Honestly, even with more specific prompting, it recommended me the same movies as the previous ones. There were a few more suggestions, but still. But at the end, it asks **would you prefer one that leans more towards supernatural horror, psychological breakdown, or cult/paranormal themes**. Perhaps if I answered that, I could have more unique responses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df3a3e-82e0-406f-a8d7-8cf27345973c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bfad46-f19f-45bf-a5af-099e38240fa4",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "</br>\n",
    "\n",
    "I definitely see the value in learning different prompting techniques—it’s a great way to steer AI toward more specific or useful responses. That said, after trying the second prompting technique, I didn’t notice a huge difference in the output. But to be fair, that might have more to do with the prompt I was using than the techniques themselves. There are only so many unique responses you can get when you're asking for movie recommendations. For example, if I say I liked Hereditary, it’s no surprise that the model suggests Midsommar, since both are by the same director and share a similar tone.\n",
    "</br>\n",
    "\n",
    "I can see these techniques being a lot more powerful in other contexts—especially for tasks that require detailed instructions, like giving directions or writing code. In those cases, the structure and phrasing of a prompt can really change the outcome. Ultimately, I think it comes down to experimenting and figuring out what works best for the type of response you’re trying to get. Prompting is definitely a skill, and like anything else, it improves the more you play around with it.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
