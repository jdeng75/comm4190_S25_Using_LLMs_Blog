{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b12c3adc-8c2e-458c-86e3-ce787bff2eca",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"AI Hallucinations\"\n",
    "description: \"not everything is correct!\"\n",
    "author: \"Jenn Deng\"\n",
    "date: \"2/3/2024\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - prompting\n",
    "  - logic\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44874cb-0c98-4a77-8e49-c4e46c9d6f80",
   "metadata": {},
   "source": [
    "## AI Hallucinations\n",
    "* Hallucinations in AI happens when there are incorrect or misleading results generated by AI models. This happens when an AI system makes false assumptions or uses biased data to generate information.\n",
    "  \n",
    "> How do hallucinations happen?\n",
    "> * not enough training data\n",
    "> * biased training data\n",
    "> * wrong assumptions\n",
    "> * processing errors\n",
    "\n",
    "* Sometimes, there's wrong code or images that pop up. It's up to us to choose whether the information is right or wrong. Let's look at a few examples that a few people have encountered in the past. </br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13d73f9-24df-48c8-a1ab-69cf94a356a8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c84879-0081-4981-992a-aaf136419fc1",
   "metadata": {},
   "source": [
    "### Reddit time!\n",
    "\n",
    "I took a quick look on Reddit to dig for some examples of ChatGPT hallucinations. I wanted to see if they still existed, since some of the posts were 1-2 years ago. </br>\n",
    "</br>\n",
    "#### Water Problem\n",
    "\n",
    "<img src=\"SS1.png\" width=\"100%\"/>\n",
    "\n",
    "**Jenn's update:** Well, I think they fixed this up.\n",
    "\n",
    "<img src=\"SS2.png\" width=\"100%\"/>\n",
    "\n",
    "Ok, well... let's check on Reddit for some more ChatGPT hallucinations\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "#### New Jersey Devils Example\n",
    "</br>\n",
    "\n",
    "<img src=\"SS3.png\" width=\"100%\"/>\n",
    "\n",
    "**Jenn's update:** It's interesting how they try to reference the information, but still got the information wrong.\n",
    "\n",
    "<img src=\"SS4.png\" width=\"100%\"/>\n",
    "\n",
    "**Ah-HAH!** One down. Let's try another one. Now, I'm not an organic chemistry person, but let's see if this one has updated from 2 years ago.\n",
    "\n",
    "\n",
    ">3. \n",
    "> <img src=\"SS5.png\" width=\"100%\"/>\n",
    "> Jenn's update: Well, darn! This is sad for any organic chemistry lovers out there. ChatGPT is really bad at organic chemistry.\n",
    "> <img src=\"SS6.png\" width=\"100%\"/>\n",
    "\n",
    "#### Conclusions & Takeaways \n",
    "###### Well, it goes to show that ChatGPT, despite its various updates, still manages to get answers wrong and hallucinate. I'm not sure if they're ever going to be able to turn ChatGPT into an organic chemistry genius or be able to correctly reference sports games. However, it's interesting to see how certain questions enable incorrect results, and how humans are able to jump in and identify these mistakes."
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
