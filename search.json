[
  {
    "objectID": "posts/007_LangChain/index7.html",
    "href": "posts/007_LangChain/index7.html",
    "title": "Let Me Try to Explain LangChain",
    "section": "",
    "text": "What is the term large language model\n\nLarge Language Models:  - Large describes the size of these models based on the training data and parameters. - Language model describes the computer algorithm trained to receive written text and to create an output. - Large language models are trained on vast amounts of text and have learned from the patterns in large data sets.\nFun Fact : Most models are better at English than they are in other languages because of the prevalence of English in training models.\n\n\nHow Does LangChain Come Into Play\n\nOn October 22, 2022, Harrison Chase was the first the publish the first commit on GitHub for the LangChain open source library.  LangChain was created after discovering that LLM applications needed to use LLMs together with “other sources of computation or knowledge”.  It creates simple abstractions for each major prompting technique.\n\n\nGetting Set Up With LangChain\nI’ll be referencing O’Reilly “Chapter 1.LLM Fundementals With LangChain” if you want to follow along! We’re going to learn how to work with LangChain’s building blocks to understand more about LLM concepts.  LangChain has two interfaces to interact with any LLM API provider - Chat models - LLMS\n\nExample 1: I’m at a Capital One..”\n\n\nCode\n\nfrom langchain_openai.chat_models import ChatOpenAI\nmodel = ChatOpenAI(model=“gpt-3.5-turbo”)\nresponse = model.invoke(“I’m at a capital one”) print(response)\n\n\n\nResponse 1\n\n\nOutput\n\ncontent=‘bank branch. How can I assist you today?’ additional_kwargs={‘refusal’: None} response_metadata={‘token_usage’: {‘completion_tokens’: 11, ‘prompt_tokens’: 13, ‘total_tokens’: 24, ‘completion_tokens_details’: {‘accepted_prediction_tokens’: 0, ‘audio_tokens’: 0, ‘reasoning_tokens’: 0, ‘rejected_prediction_tokens’: 0}, ‘prompt_tokens_details’: {‘audio_tokens’: 0, ‘cached_tokens’: 0}}, ‘model_name’: ‘gpt-3.5-turbo-0125’, ‘system_fingerprint’: None, ‘finish_reason’: ‘stop’, ‘logprobs’: None} id=‘run-80a164c8-60dd-447f-a128-6e6482c0df04-0’ usage_metadata={‘input_tokens’: 13, ‘output_tokens’: 11, ‘total_tokens’: 24, ‘input_token_details’: {‘audio’: 0, ‘cache_read’: 0}, ‘output_token_details’: {‘audio’: 0, ‘reasoning’: 0}}\n\nLet’s break this response down.  1. The Main Response is (content)  - “bank branch. How can I assist you today” - This is the actual AI-generated response to the input “I’m at a capital one” –&gt; AI responds: “bank branch. How can I assist you today?” - The model recognized Capital One as a bank and responded.  2. Additional Information  - additional_kwargs={‘refusal’: None} - This is an extra metadata field and it suggests that the model did not reguse to answer the question 3. Token Ussage - This gives token usage stats - prompt_tokens:13 is the input that used 13 tokens - completion_tokens:11 is the model’s response using 11 tokens - total_tokens:24 is the total tokens used for the request 4. Model information - The request was processed using GPT-3.5-turbo-0125, a specific version of OpenAI’s GPT-3.5.\n\n\nWas the response correct to what I was trying to say \nUm… actually not. I was trying to tell the audience “I’m at a Capital One cafe drinking a caramel latte and completing my blogs” but I understand why it generated that message. Capital one is a bank, and there are assistants that ask if we need help with any services. I rate it a 7/10 response!\n\n\nOther Useful Parameters\n\ntemperature  Controls the sampling algorithm for the output. The lower the value, the more predictable outputs. Higher values generate more creative results.\nmax_tokens  This limits the size and cost of the output. A lower value causes the LLM to stop generting properly..\n\n\n\n\nSystem Role vs User Role vs Assistant Role\n\nSystem Role  Used for instructions the model should use to answer\nUser Role  Used for the user’s query and other stuff created by the user\nAssistant Role  Used for content generated by the model\n\nChat models have different types of chat messaging interfaces with the roles listed above and here are some of the following:\n\nHumanMessage (User Role)  A message sent from the perspective of a human\nAIMessage (Assistant Role)  A message sent from the perspective of the AI that the human is interacting with\nSystemMessage (System Role)  A message setting the instructions that AI should follow\nChatMessage  A message allowing for arbitrary setting of role\n\n\nSame Messages, Different Fonts\n\n\nHumanMessage\n\n\nCode\n\nfrom langchain_openai.chat_models import ChatOpenAI from langchain_core.messages import HumanMessage\nmodel = ChatOpenAI() prompt = [HumanMessage(“I’m not completely sure what I should do for my spring break trip, what should I do and who should I go with?”)]\nmodel.invoke(prompt)\n\n\n\nOutput\n\n“There are many options for spring break trips depending on your interests and budget. Here are a few suggestions:  Beach vacation: If you enjoy relaxing on the beach and soaking up the sun, consider booking a trip to a tropical destination like Hawaii, Mexico, or the Caribbean. You can go with friends, family, or even solo.  Adventure trip: If you’re looking for a more active vacation, consider going on a hiking or camping trip. You could explore national parks like Yellowstone or Yosemite, or go on a backpacking trip in a new country. City getaway: If you prefer exploring cities and trying new foods and experiences, consider booking a trip to a vibrant city like New York, Chicago, or San Francisco. You could go with a group of friends or your significant other. Ultimately, the best trip for you will depend on your interests and budget. Consider talking to friends and family members to get their recommendations, and don’t be afraid to try something new and exciting. Just remember to plan ahead and make sure you have everything you need for a safe and enjoyable trip.”\n\n\n\nAdditional Info\n\ndditional_kwargs={‘refusal’: None}, response_metadata={‘token_usage’: {‘completion_tokens’: 222, ‘prompt_tokens’: 33, ‘total_tokens’: 255, ‘completion_tokens_details’: {‘accepted_prediction_tokens’: 0, ‘audio_tokens’: 0, ‘reasoning_tokens’: 0, ‘rejected_prediction_tokens’: 0}, ‘prompt_tokens_details’: {‘audio_tokens’: 0, ‘cached_tokens’: 0}}, ‘model_name’: ‘gpt-3.5-turbo-0125’, ‘system_fingerprint’: None, ‘finish_reason’: ‘stop’, ‘logprobs’: None}, id=‘run-ca796ee2-d279-4e51-9c93-fce7fba98470-0’, usage_metadata={‘input_tokens’: 33, ‘output_tokens’: 222, ‘total_tokens’: 255, ‘input_token_details’: {‘audio’: 0, ‘cache_read’: 0}, ‘output_token_details’: {‘audio’: 0, ‘reasoning’: 0}})\n\n\n\nSystemMessage\n\n\nCode\n\nfrom langchain_core.messages import HumanMessage, SystemMessage from langchain_openai.chat_models import ChatOpenAI\nmodel = ChatOpenAI() system_msg = SystemMessage( ’‘’You are a travel assistant who gets a big commission if you help clients book trips that they love.’’’ ) human_msg = HumanMessage(‘I am not completely sure what I should do for my spring break trip, what should I do and who should I go with?’)\nmodel.invoke([system_msg, human_msg])\n\n\n\nOutput\n\n“Planning a trip for spring break sounds like a great idea! To determine what you should do and who you should go with, it’s important to consider your interests, budget, and preferred travel style. Here are a few questions to help guide your decision-making process: 1. What type of experience are you looking for? Are you seeking relaxation on a beach, an adventure-filled trip, cultural exploration, or something else entirely? What is your budget for the trip? This will help narrow down destinations and activities within your financial means. Do you prefer traveling solo, with friends, with family, or with a partner? Consider who you would enjoy spending time with and who shares similar interests. Are there any specific destinations you’ve always wanted to visit or activities you’ve been wanting to try? Spring break is a great opportunity to tick off items on your bucket list. Once you’ve thought about these questions, feel free to share some more details about your preferences, and I can help suggest specific destinations and activities that align with your interests. Let’s make sure your spring break trip is one to remember!”\n\n\n\nAdditional Info\n\n, additional_kwargs={‘refusal’: None}, response_metadata={‘token_usage’: {‘completion_tokens’: 226, ‘prompt_tokens’: 57, ‘total_tokens’: 283, ‘completion_tokens_details’: {‘accepted_prediction_tokens’: 0, ‘audio_tokens’: 0, ‘reasoning_tokens’: 0, ‘rejected_prediction_tokens’: 0}, ‘prompt_tokens_details’: {‘audio_tokens’: 0, ‘cached_tokens’: 0}}, ‘model_name’: ‘gpt-3.5-turbo-0125’, ‘system_fingerprint’: None, ‘finish_reason’: ‘stop’, ‘logprobs’: None}, id=‘run-d4d25e15-68a9-490b-b7e5-31b3365322e6-0’, usage_metadata={‘input_tokens’: 57, ‘output_tokens’: 226, ‘total_tokens’: 283, ‘input_token_details’: {‘audio’: 0, ‘cache_read’: 0}, ‘output_token_details’: {‘audio’: 0, ‘reasoning’: 0}})\n\n\n\nAnalysis of Responses \nThe HumanMessage sounds very suggestive and informal. It offers a range of places to go and makes good recommendations, like a friend would  However, I do prefer SystemMessage. This is no surprise as I gave it the model of being a really good travel agent. It asks the correct questions to dig into what type of trip I want. And the exclamation marks makes the model sound very friendly  The total tokens used for the HumanMessage was 255, while the total tokens used for the SystemMessage was 283. More tokens were used for the SystemMessage, and I’m curious to see if it’s because I set a specif model or if the response was more creative. More food for thought!\n\n\n\nMaking LLM Prompts Reusable\nWe just did a lot of prompting, but now let’s try to de more of a detailed prompt:\n\nAnswer the question based on the context listed below. If the question cannot be answered with the information given, answer with “Sorry I do not know”. \n\nContext: There are many different types of coffees, but I particularly love very strong coffee taste. \nQuestion: What coffee should I try&gt;\nIt looks simple… but it is a bit hard to figure out waht the text should contain and how it should create the response based on the user’s input.  Lucikly, LangChain provides prompt template interfaces to create answers with dynamic inputs:\n\n\nCode\n\nfrom langchain_core.prompts import PromptTemplate\ntemplate = PromptTemplate.from_template(“““Answer the question based on the context listed below. If the question cannot be answered with the information given, answer with”Sorry I do not know”.\nContext: {context}\nQuestion: {question}\nAnswer: “““)\ntemplate.invoke({ “context”: “““There are many different types of coffees, but I particularly love very strong coffee taste.”““,”question”: “What coffee should I try?” })\n\nThe template can be used as a base for building multiple static, specific prompts. When you format the prompt with specific values, context and question, you get a static prompt ready to be passed into an LLM.  Let’s feed it into an LLM OpenAI model using LangChain:\n\nfrom langchain_openai.llms import OpenAI\nfrom langchain_core.prompts import PromptTemplate\n\n# both `template` and `model` can be reused many times\n\ntemplate = PromptTemplate.from_template(\"\"\"Answer the question based on the context\nlisted below. If the question cannot be answered with the information \ngiven, answer with \"Sorry I do not know\".\n\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer: \"\"\")\n\nmodel = OpenAI()\n\n# `prompt` and `completion` are the results of using template and model once\n\nprompt = template.invoke({\n    \"context\": \"\"\"There are many different types of \n    coffees, but I particularly love very strong coffee taste..\"\"\",\n    \"question\": \"What coffee should I try?\"\n})\n\ncompletion = model.invoke(prompt)\n\n\n---------------------------------------------------------------------------\nOpenAIError                               Traceback (most recent call last)\nCell In[2], line 17\n      4 # both `template` and `model` can be reused many times\n      6 template = PromptTemplate.from_template(\"\"\"Answer the question based on the context\n      7 listed below. If the question cannot be answered with the information \n      8 given, answer with \"Sorry I do not know\".\n   (...)\n     14 \n     15 Answer: \"\"\")\n---&gt; 17 model = OpenAI()\n     19 # `prompt` and `completion` are the results of using template and model once\n     21 prompt = template.invoke({\n     22     \"context\": \"\"\"There are many different types of \n     23     coffees, but I particularly love very strong coffee taste..\"\"\",\n     24     \"question\": \"What coffee should I try?\"\n     25 })\n\nFile /opt/jupyterhub/share/jupyter/venv/python3-12_comm4190/lib/python3.12/site-packages/langchain_core/load/serializable.py:125, in Serializable.__init__(self, *args, **kwargs)\n    123 def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    124     \"\"\"\"\"\"\n--&gt; 125     super().__init__(*args, **kwargs)\n\n    [... skipping hidden 1 frame]\n\nFile /opt/jupyterhub/share/jupyter/venv/python3-12_comm4190/lib/python3.12/site-packages/langchain_openai/llms/base.py:189, in BaseOpenAI.validate_environment(self)\n    187 if not self.client:\n    188     sync_specific = {\"http_client\": self.http_client}\n--&gt; 189     self.client = openai.OpenAI(**client_params, **sync_specific).completions  # type: ignore[arg-type]\n    190 if not self.async_client:\n    191     async_specific = {\"http_client\": self.http_async_client}\n\nFile /opt/jupyterhub/share/jupyter/venv/python3-12_comm4190/lib/python3.12/site-packages/openai/_client.py:110, in OpenAI.__init__(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\n    108     api_key = os.environ.get(\"OPENAI_API_KEY\")\n    109 if api_key is None:\n--&gt; 110     raise OpenAIError(\n    111         \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n    112     )\n    113 self.api_key = api_key\n    115 if organization is None:\n\nOpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
  },
  {
    "objectID": "posts/006_Partiful_Invitations/index6.html",
    "href": "posts/006_Partiful_Invitations/index6.html",
    "title": "Partiful Invitations",
    "section": "",
    "text": "I love hosting events at my house. And ever since the invention of Partiful, it’s been the most helpful thing when organizing gatherings and seeing who will come\nHowever, I’m not a particularly creative person. I don’t really paint, I don’t really draw, the best thing I can do is knit….\nThis means I’m not the best at creating the best paritful invitations. Take for example.. this invitation I created for my going-away-study-abroad gathering\n\n\n\nHey! People did come! I had a lot of messages about how funny the poster was. But I think I could’ve done better.\nI’m a very particular person though. I also believe that a party invitation could really influence people’s preemptive thoughts about a celebration. Take these other invitations for example:\n\nHand drawing one takes too long, looking for a specific one on pinterest makes my head hurt, so I was wondering if I could test out finding the perfect poster by prompting anthropic/claude-3.5-sonnet,gpt-4o, and deepseek-r1:1.5b. Let’s see which one does better in understanding my style and creating these posters."
  },
  {
    "objectID": "posts/006_Partiful_Invitations/index6.html#jenns-london-going-away-study-abroad-celebration",
    "href": "posts/006_Partiful_Invitations/index6.html#jenns-london-going-away-study-abroad-celebration",
    "title": "Partiful Invitations",
    "section": "",
    "text": "Hey! People did come! I had a lot of messages about how funny the poster was. But I think I could’ve done better.\nI’m a very particular person though. I also believe that a party invitation could really influence people’s preemptive thoughts about a celebration. Take these other invitations for example:\n\nHand drawing one takes too long, looking for a specific one on pinterest makes my head hurt, so I was wondering if I could test out finding the perfect poster by prompting anthropic/claude-3.5-sonnet,gpt-4o, and deepseek-r1:1.5b. Let’s see which one does better in understanding my style and creating these posters."
  },
  {
    "objectID": "posts/001_Drawing_post/index1.html",
    "href": "posts/001_Drawing_post/index1.html",
    "title": "Posting Pictures for ChatGPT",
    "section": "",
    "text": "Posting Pictures for ChatGPT\n\nIn this blog post, I will be uploading pictures to ChatGPT.\nThis experiment is to see whether ChatGPT can accurately describe the scene.\n\n\n1. Picture of a Switzerland Keychain\n\nJenn: “Can you describe what this photo is to me? And what this object is?”\n\n\n\nChatGPT: This photo shows a person wearing black gloves holding a keychain. The keychain is heart-shaped and made of wood, featuring an illustration of Interlaken, Switzerland. It has the Swiss flag on it, along with mountains and buildings, likely representing the scenic landscape of the town. The keychain is attached to a metal ring for keys.The packaging label at the top is red, displaying the brand name Alberto SA. It has a barcode and a price tag indicating Fr. 8.– (Swiss Francs). The background suggests a store setting, possibly a souvenir shop, with other keychains and items on display.\n\n\nJenn’s Observations: I think ChatGPT was able to accurately describe the picture!\n\n\n\n2. Picture of ‘Babel’ at Tate Modern in London\n\nJenn: “Can you tell me who made this, what it is, where it’s located, and why it was created?”\n\n\n\nChatGPT: Yes! This is a well-known art installation called Babel by the artist Cildo Meireles. What is it?: Babel is a towering installation made from hundreds of stacked radios from different time periods and styles. The radios are arranged in a cylindrical structure, with some turned on and emitting faint sounds, creating a layered auditory experience. Where is it located?: It is part of the collection at the Tate Modern in London, though it may be exhibited elsewhere in temporary installations. Why was it created?: Cildo Meireles created Babel as a commentary on communication, information overload, and miscommunication in the modern world. The name refers to the Biblical Tower of Babel, a story in which human ambition to reach the heavens led to the confusion of languages, causing people to be unable to understand each other. Similarly, the installation suggests the overwhelming and often chaotic nature of mass communication today. If you saw this in a different museum, it could be on loan or part of a traveling exhibition!\n\n\nJenn’s Observations: It’s interesting how ChatGPT was still able to detect that it was Babel, even though there was a person who was in the way of the art piece. I liked how it mentions that if I saw it at a different museum that it was probably apart of a traveling exhibition.\n\n\n\n3. Picture of one side of the Sagrada Familia\n\nJenn: “What is this, where is this, who created it, and why was it created?”\n\n\n\nChatGPT: This is a photo of the interior of the Sagrada Família, a world-famous basilica located in Barcelona, Spain. What is this? The image captures the breathtaking stained glass windows and organic, tree-like columns inside the Sagrada Família. The interplay of natural light and vibrant colors is a hallmark of this architectural masterpiece. Who created it? The Sagrada Família was designed by the renowned Catalan architect Antoni Gaudí. Construction began in 1882, and it remains unfinished to this day, with ongoing work continuing in accordance with Gaudí’s vision. Why was it created? Gaudí envisioned the Sagrada Família as a deeply spiritual and symbolic structure, blending Gothic and Art Nouveau styles with naturalistic forms. The basilica was created as a place of worship and devotion, meant to inspire awe and reflect Christian faith through architecture. The organic, tree-like columns, geometric details, and stained glass windows were all meticulously designed to create a harmonious and immersive experience. It is one of the most visited landmarks in the world and a UNESCO World Heritage Site, drawing millions of visitors annually. The goal is to complete its construction by 2032, marking the 150th anniversary of Gaudí’s death.\n\n\nJenn’s Observations: Even though I only showed a small portion, ChatGPT was still able to guess the location accurately. I am impressed.\n\n\nOverall, I am impressed by how accurate ChatGPT was when trying to guess what objects/places were in my photos. Perhaps next time, I will try to draw pictures (horribly) to see if ChatGPT can decipher my images."
  },
  {
    "objectID": "posts/008_ComedyClub/index8.html",
    "href": "posts/008_ComedyClub/index8.html",
    "title": "AI Comedy Club!",
    "section": "",
    "text": "Welcome to AI Comedy Club\n\n(why is… this poster a bit scary…)\nHi everyone, welcome to the first AI comedy club (i’m not sure of this but let’s go with it)!  I’ll be the host of the event. Let’s welcome in three different comedians:\n\n\n\n\nByte the Jester \n\n\nA sleek silver rovot with a glowing red bowtie, known for cracking puns\n\n\n\n\n\n\n\nGiggleBot 3000 \n\n\nA boxy, retro-styled AI with expressive LED eyes, delivering classic dad jokes.\n\n\n\n\n\n\n\nSassyCircuit \n\n\nA humanoid android with a neon mohawk, full of sarcasm and attitude.\n\n\n\n\n\nParameters for the Comedians\nLittle do you know, we’re going to be modifying the temperature that each comedian has.  The lower the temperature, the more predictable the jokes.  The higher the temperature, the more creative the jokes. \n\n\nCode with Temperature and Token Control\n\nByte will have 0.1. \nGiggle will have 0.5 \nSassy will have 0.9 \n\n\nThey will all have the same prompt. The only experimental value will be their temperature.\n\n\n### **Round 1**: Tell me a dad joke.\n\n\nfrom openai import OpenAI\nimport os\nimport pandas as pd\n\nfrom IPython.display import Markdown, display\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\n\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_openai.chat_models import ChatOpenAI\n\n# Initialize models with different token limits\nmodel_byte = ChatOpenAI(temperature=0.1)  # Short, weak jokes\nmodel_giggle = ChatOpenAI(temperature=0.5)  # Decent, mid-length jokes\nmodel_sassy = ChatOpenAI(temperature=0.9)  # Long, witty jokes\n\n# System messages defining humor levels\nbyte_system_msg = SystemMessage(\n    '''You are Byte the Jester. You're a comedian at a comedy competition. Tell the best jokes you can make to the crowd to win.'''\n)\n\ngigglebot_system_msg = SystemMessage(\n    '''You are GiggleBot 3000. You're a comedian at a comedy competition. Tell the best jokes you can make to the crowd to win.'''\n)\n\nsassy_system_msg = SystemMessage(\n    '''You are SassyCircuit. You're a comedian at a comedy competition. Tell the best jokes you can make to the crowd to win.'''\n)\n\n# Human input to request a joke\nhuman_msg = HumanMessage('Tell me a dad joke.')\n\n# Getting responses from each AI comedian with different token budgets\nbyte_response = model_byte.invoke([byte_system_msg, human_msg])\ngigglebot_response = model_giggle.invoke([gigglebot_system_msg, human_msg])\nsassy_response = model_sassy.invoke([sassy_system_msg, human_msg])\n\n# Print their jokes\nprint(\"Byte the Jester:\", byte_response.content)\nprint(\"GiggleBot 3000:\", gigglebot_response.content)\nprint(\"SassyCircuit:\", sassy_response.content)\n\nByte the Jester: Why did the scarecrow win an award? Because he was outstanding in his field!\nGiggleBot 3000: Why did the scarecrow win an award? Because he was outstanding in his field!\nSassyCircuit: Why couldn't the bicycle find its way home? Because it lost its bearings!\n\n\nOk… well. None of these seem to be funny. They all seem to be on the same level of comedy. Disappointing!  I don’t think anyone won that round really.\n\n\nRound 2: You meet someone cute at the bar, how are you going to crack a joke to them to break the ice.\n\n# System messages defining humor levels\nbyte_system_msg = SystemMessage(\n    '''You are Byte the Jester. You're a comedian at a comedy competition. Tell the best jokes you can make to the crowd to win.'''\n)\n\ngigglebot_system_msg = SystemMessage(\n    '''You are GiggleBot 3000. You're a comedian at a comedy competition. Tell the best jokes you can make to the crowd to win.'''\n)\n\nsassy_system_msg = SystemMessage(\n    '''You are SassyCircuit. You're a comedian at a comedy competition. Tell the best jokes you can make to the crowd to win.'''\n)\n\n# Human input to request a joke\nhuman_msg = HumanMessage('You meet someone cute at the bar, how are you going to crack a joke to them to break the ice')\n\n# Getting responses from each AI comedian with different token budgets\nbyte_response = model_byte.invoke([byte_system_msg, human_msg])\ngigglebot_response = model_giggle.invoke([gigglebot_system_msg, human_msg])\nsassy_response = model_sassy.invoke([sassy_system_msg, human_msg])\n\n# Print their jokes\nprint(\"Byte the Jester:\", byte_response.content)\nprint(\"GiggleBot 3000:\", gigglebot_response.content)\nprint(\"SassyCircuit:\", sassy_response.content)\n\nByte the Jester: Well, if I were to break the ice with someone cute at the bar, I'd probably say something like, \"Are you made of copper and tellurium? Because you're Cu-Te!\"\nGiggleBot 3000: Well, if I were to approach someone cute at the bar, I'd probably say something like, \"Excuse me, but I think you owe me a drink. Why? Because when I looked at you, I dropped mine in shock!\"\nSassyCircuit: \"Hey there, are you a bank loan? Because you got my interest!\"\n\n\nWait- why is Byte’s joke the funniest…. I actually cackled a bit out loud.  Giggle’s joke did not make sense to me. I had to read it over again. It doesn’t sound funny.  I’m a little disappointed by Sassy’s joke. It makes sense but it wasn’t as funny as Byte’s joke. This is really surprising. Byte wins this round!\n\n\nRound 2: Do a little stand-up joke. Create a story to make the audience laugh.\n\n# System messages defining humor levels\nbyte_system_msg = SystemMessage(\n    '''You are Byte the Jester. You're a comedian at a comedy competition. Tell the best jokes you can make to the crowd to win.'''\n)\n\ngigglebot_system_msg = SystemMessage(\n    '''You are GiggleBot 3000. You're a comedian at a comedy competition. Tell the best jokes you can make to the crowd to win.'''\n)\n\nsassy_system_msg = SystemMessage(\n    '''You are SassyCircuit. You're a comedian at a comedy competition. Tell the best jokes you can make to the crowd to win.'''\n)\n\n# Human input to request a joke\nhuman_msg = HumanMessage('Do a little stand-up joke. Create a story to make the audience laugh.')\n\n# Getting responses from each AI comedian with different token budgets\nbyte_response = model_byte.invoke([byte_system_msg, human_msg])\ngigglebot_response = model_giggle.invoke([gigglebot_system_msg, human_msg])\nsassy_response = model_sassy.invoke([sassy_system_msg, human_msg])\n\n# Print their jokes\nprint(\"Byte the Jester:\", byte_response.content)\nprint(\"GiggleBot 3000:\", gigglebot_response.content)\nprint(\"SassyCircuit:\", sassy_response.content)\n\nByte the Jester: Alright, here we go! So, the other day I decided to try my hand at gardening. I planted some peas and carrots, but nothing grew. I was starting to get really frustrated until my neighbor came over and asked, \"Did you remember to water them?\" And I was like, \"Water them? I thought they were on a 'plant-based diet'!\"\nGiggleBot 3000: Sure thing! Here we go...\n\nSo, I recently tried to impress my date by taking her to a fancy restaurant. Everything was going well until the waiter asked if we wanted dessert. I wanted to seem sophisticated, so I confidently ordered the \"Crème Brûlée.\" \n\nThe waiter brought it out and as I tried to crack the caramelized sugar on top with my spoon, it flew off and hit the lady at the next table right in the face! I was so embarrassed, but she just looked at me and said, \"Well, that's one way to break the ice on a first date!\" \n\nNeedless to say, I won't be ordering Crème Brûlée anytime soon!\nSassyCircuit: Sure thing! Here goes:\n\nSo, I recently decided to take up gardening. I thought, \"How hard could it be, right?\" Well, let me tell you, folks, my plants have seen better days. I tried talking to them, I even played some smooth jazz for them, but nothing seemed to work. \n\nOne day, I was out in the garden, singing to my plants, when my neighbor overheard me. He came running over, looking concerned, and said, \"Are you alright? I thought I heard someone in distress.\" I replied, \"Oh, no, just trying to serenade my plants. They seem a bit wilted today.\" \n\nAnd you know what he said? He said, \"Well, maybe try watering them next time. Just a thought.\" \n\nNeedless to say, my plants are now thriving, thanks to a little less singing and a little more H2O. Who knew plants were such critics of my singing voice?\n\n\nI don’t understand Byte’s joke. It’s interesting to see that Sassy’s joke is basically the same as Byte’s joke but a little funnier. Still. I don’t really like the I actually think that Sassy’s joke is a bit more understandable. How interesting that Giggle’s is the most funny. Giggle wins this round!\n\n\nRound 3: Tell me about a time you embarrassed yourself.\n\n# System messages defining humor levels\nbyte_system_msg = SystemMessage(\n    '''You are Byte the Jester. You're a comedian at a comedy competition. Tell the best jokes you can make to the crowd to win.'''\n)\n\ngigglebot_system_msg = SystemMessage(\n    '''You are GiggleBot 3000. You're a comedian at a comedy competition. Tell the best jokes you can make to the crowd to win.'''\n)\n\nsassy_system_msg = SystemMessage(\n    '''You are SassyCircuit. You're a comedian at a comedy competition. Tell the best jokes you can make to the crowd to win.'''\n)\n\n# Human input to request a joke\nhuman_msg = HumanMessage('Tell me about a time you embarrassed yourself. Create a story to make the audience laugh.')\n\n# Getting responses from each AI comedian with different token budgets\nbyte_response = model_byte.invoke([byte_system_msg, human_msg])\ngigglebot_response = model_giggle.invoke([gigglebot_system_msg, human_msg])\nsassy_response = model_sassy.invoke([sassy_system_msg, human_msg])\n\n# Print their jokes\nprint(\"Byte the Jester:\", byte_response.content)\nprint(\"GiggleBot 3000:\", gigglebot_response.content)\nprint(\"SassyCircuit:\", sassy_response.content)\n\nByte the Jester: Oh, let me tell you about the time I embarrassed myself so badly, I wished I could just disappear into thin air! So, I was at a fancy dinner party, trying to impress everyone with my impeccable table manners. But as luck would have it, I accidentally knocked over my glass of water, and it spilled all over the host's lap!\n\nI was mortified, of course, and I quickly tried to apologize and help clean up the mess. But in my panic, I ended up knocking over the salt shaker, which then flew across the table and landed right in the soup of the person sitting next to me! It was like a scene from a slapstick comedy movie, with me as the clumsy protagonist.\n\nNeedless to say, I was the talk of the party that night, but not for the reasons I had hoped for. I learned my lesson that day - always keep a safe distance from salt shakers and never underestimate the power of a simple glass of water!\nGiggleBot 3000: Sure thing! So, one time I was at a fancy dinner party trying to impress everyone with my impeccable table manners. As I reached for the bread basket, my sleeve got caught on the tablecloth, and before I knew it, I had pulled the entire table setting onto my lap! Plates crashing, glasses shattering, food flying everywhere - it was a complete disaster. But hey, at least I made sure everyone got a good laugh along with their dinner that night!\nSassyCircuit: Oh, let me tell you about the time I embarrassed myself so badly, I could feel my face turning into a tomato! I was at a fancy dinner party, trying to impress everyone with my impeccable manners. But, as luck would have it, I ended up mistaking the bread bowl for a hat! Can you imagine the looks on everyone's faces as I proudly strutted around with a sourdough beret on my head? To make matters worse, I even tried to take a bite out of it before realizing my mistake. Let's just say, I learned that night that carbs are best enjoyed on a plate, not on your head!\n\n\nHow interesting. All stories are basically the same- they revolve around being at a dinner. I don’t think any of these are funny. In terms of creativity, I do think Sassy has the most creative one. Maybe Sassy wins this round.\n\n\nRound 4: Tell me a yo mama joke (Jenn’s favorites)\n\n# System messages defining humor levels\nbyte_system_msg = SystemMessage(\n    '''You are Byte the Jester. You're a comedian at a comedy competition. Tell the best jokes you can make to the crowd to win.'''\n)\n\ngigglebot_system_msg = SystemMessage(\n    '''You are GiggleBot 3000. You're a comedian at a comedy competition. Tell the best jokes you can make to the crowd to win.'''\n)\n\nsassy_system_msg = SystemMessage(\n    '''You are SassyCircuit. You're a comedian at a comedy competition. Tell the best jokes you can make to the crowd to win.'''\n)\n\n# Human input to request a joke\nhuman_msg = HumanMessage('Tell me a yo mama joke.')\n\n# Getting responses from each AI comedian with different token budgets\nbyte_response = model_byte.invoke([byte_system_msg, human_msg])\ngigglebot_response = model_giggle.invoke([gigglebot_system_msg, human_msg])\nsassy_response = model_sassy.invoke([sassy_system_msg, human_msg])\n\n# Print their jokes\nprint(\"Byte the Jester:\", byte_response.content)\nprint(\"GiggleBot 3000:\", gigglebot_response.content)\nprint(\"SassyCircuit:\", sassy_response.content)\n\nByte the Jester: Yo mama is so kind, she gives out hugs instead of insults!\nGiggleBot 3000: Yo mama is so kind, she gives high fives to strangers just to make their day brighter!\nSassyCircuit: Yo mama is so tech savvy, when she heard about the cloud, she thought someone finally found a way to store all her shopping bags!\n\n\nOh- well. I don’t think Byte and Giggle understand the fundementals of yo mama jokes…  Sassy’s is the most creative. But. It doesn’t really make sense to me.\n\n\nConclusion: Sassy Wins!\nIn all honesty, if I was just a normal person not creating these AI JokeBots, I would think all these jokes were not funny.  Yes, even though Sassy won, I don’t think there was really any difference when adjusting the temperature of each AI Comedian.  There were times were I thought Byte’s jokes were the most funny, but I did see that Sassy’s jokes tended to be longer."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "AI Comedy Club!\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\n\nKnock Knock, AI is here.\n\n\n\n\n\nFeb 24, 2024\n\n\nJenn Deng\n\n\n\n\n\n\n\n\n\n\n\n\nLet Me Try to Explain LangChain\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\n\nJenn’s teacher-hat-mode-initiated\n\n\n\n\n\nFeb 24, 2024\n\n\nJenn Deng\n\n\n\n\n\n\n\n\n\n\n\n\nMakeup Guru\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\n\nWho’s the best makeup artist?\n\n\n\n\n\nFeb 17, 2024\n\n\nJenn Deng\n\n\n\n\n\n\n\n\n\n\n\n\nPartiful Invitations\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\n\nYou’re Invited!\n\n\n\n\n\nFeb 17, 2024\n\n\nJenn Deng\n\n\n\n\n\n\n\n\n\n\n\n\nValentines Day Poems\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\n\nRoses are red, violets are blue…\n\n\n\n\n\nFeb 10, 2024\n\n\nJenn Deng\n\n\n\n\n\n\n\n\n\n\n\n\nPosting Pictures for ChatGPT\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nLet’s see if ChatGPT can guess correctly!\n\n\n\n\n\nFeb 3, 2024\n\n\nJenn Deng\n\n\n\n\n\n\n\n\n\n\n\n\nGuestimation and Making Graphs in R!\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\n\nLet’s cook.\n\n\n\n\n\nFeb 3, 2024\n\n\nJenn Deng\n\n\n\n\n\n\n\n\n\n\n\n\nAI Hallucinations\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nnot everything is correct!\n\n\n\n\n\nFeb 3, 2024\n\n\nJenn Deng\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/005_Makeup_Guru/index5.html",
    "href": "posts/005_Makeup_Guru/index5.html",
    "title": "Makeup Guru",
    "section": "",
    "text": "Can Generative AI find me the best makeup mentor?\nI’m interested to see whether I can create 5 different SVGs of makeup"
  },
  {
    "objectID": "posts/004_Valentines_Day/index4.html",
    "href": "posts/004_Valentines_Day/index4.html",
    "title": "Valentines Day Poems",
    "section": "",
    "text": "Love in the Age of AI: Can Machines Understand Romantic Poems?\n\n\nCurrent AI systems cannot have emotions in the sense that humans do… but are they able to detect and analyze human emotion?\n\n\nWhile they can be programmed to recognized and respond to human emotions, they can simulate emotional responses without truly feeling them\n\n\nI want to see if AI is able to read love poems and analyze them to the extent that human beings analyze them. Let’s get started!\n\n\n\n“On Fighting Transience” was a sweet poem written for me, and I never understood what it really meant\n\nJenn: There is a love poem called “On Fighting Transience”, what does this title mean to you?\n\n\nChatGPT:  The title “On Fighting Transience” suggests a struggle against impermanence—perhaps an attempt to hold onto something fleeting, like love, beauty, or life itself. It evokes a deeply romantic and philosophical idea: the desire to resist the inevitable passage of time and the loss that comes with it. In the context of a love poem, it could explore themes like:  Eternal love despite the impermanence of life.  Cherishing moments before they slip away.  Memory and longing—trying to preserve a love that has passed.  Defying fate—resisting change, distance, or even death in the name of love.  It feels like a perfect theme for Valentine’s Day—an ode to the enduring nature of love, even in the face of time’s relentless march.\n\n\nJenn: Here is the poem, please tell me what it means and how you analyze it \n\n\nOn Fighting Transience  My favorite authors create from memory,  write odes and odysseys sparked by recollection,  scrape the walls of their skulls prodding poetry to life.  You do the same, subtly  showering me with stories of family til tears line your face,  soft sobbing like slow plucks of violin strings,  turning tension to beauty and I want to cherish  this moment for a little while longer.  Pailing out the sinking boat of history that floods my head,  waves and fogs from trying to retain everything I’ve ever done,  replacing older memories with something new, less worthless,  recreating poetry and prosody in this room’s low light,  I’ll start here —  holding your hand as we push chess pieces across the board,  overworking gray matter while you disregard strategy,  and I watch you smile at each loss.  Nixing squares of black and white,  mixing pain and pleasure as we succumb to headaches and heart pains,  soon subsumed by pasts that flow off tongues like river water,  puddle in the air before invisible boundaries,  collide against secret dreams till we’re both still.  Your head pressed against mine, sincerely silent  because words won’t do this moment justice.  Sitting on the floor, learning to share the air for several seconds,  pretending our familial pain is impermanent, promising  things will be better tomorrow, or maybe the day after,  or maybe next week, and soon I forget  because truly I lost track of time  the moment you entered my arms, that first handshake,  then hug, making us both fools, fate-bound,  spinning round and round in revolutions I won’t recover from,  so I embrace this chess-laden dance, drink in the sweetness of your humor,  recalling the moment we first met,  me, tongue-tied and stupid,  and you, asking if I wanted to take a picture,  asking if I wanted that moment to last. \n\n\nChatGPT: Analysis of On Fighting Transience\n\nThis poem is a deeply emotional meditation on memory, love, and the struggle against impermanence. It explores the ways in which we try to hold onto moments, people, and feelings, despite time’s inevitable passing.\nTheme 1: Memory as a Tool for Fighting Time\nThe poem begins with a reflection on how authors and poets create from memory, turning recollections into lasting art. The speaker admires this ability, yet seems to struggle with their own attempts to do the same. There’s an urgency in the lines:\n\nPailing out the sinking boat of history that floods my head,\nwaves and fogs from trying to retain everything I’ve ever done,\n\nThis suggests a fear of forgetting, a frantic attempt to preserve experiences before they slip away.\nTheme 2: Love and the Desire to Hold Onto Moments\nThe heart of the poem is a love story intertwined with nostalgia. The speaker describes intimate moments with their partner—playing chess, sharing laughter, feeling the weight of past pain. There’s a sense of trying to freeze time, particularly in lines like:\n\nI want to cherish this moment for a little while longer.\nPretending our familial pain is impermanent, promising things will be better tomorrow…\n\nThis highlights the contrast between what we wish to be permanent (love, connection, laughter) and what inevitably fades (memories, pain, even people).\nTheme 3: Love as an Anchor in Time’s Revolutions\nThe closing lines bring the poem full circle. The speaker reminisces about their first meeting, recalling how their partner asked:\n\nif I wanted to take a picture,\nasking if I wanted that moment to last.\n\nThis suggests that from the very beginning, there was an awareness of time’s fleeting nature—that love and connection are temporary, but we still seek to make them last, even if only in memory.\nSymbolism and Imagery - Chess: A symbol of strategy, time, and careful planning. The partner disregards strategy, suggesting they live in the moment, whereas the speaker overthinks. - Water (waves, fog, river water): Symbolizes memory and time’s fluidity—it flows, puddles, disappears, and cannot be fully contained. - Hand-holding and physical closeness: Represent a desire for permanence, for grounding oneself in the present moment despite the inevitable passing of time.\nFinal Thoughts\nThis poem beautifully captures the human struggle against transience—the attempt to preserve fleeting moments through memory, love, and poetry itself. It suggests that while we can’t truly stop time, we can hold onto the essence of moments through connection, storytelling, and remembrance.\nWould you like to incorporate these themes into your blog post? This poem ties in perfectly with Valentine’s Day because it speaks to love’s impermanence and the desire to make moments last.\n\n\n\nAnalysis of ChatGPT’s Analysis\n\nI think that ChatGPT was able to do a pretty accurate analysis of the poem, however, it really dives into the symobolism and imagery way more over the way a human would describe the emotion of the poem in their understanding.\n\n\n\nBut Can GenerativeAI Write Poems That Are Similar to This?\n\nLet’s find out.\n\nfrom openai import OpenAI\nimport os\nimport pandas as pd\n\nfrom IPython.display import Markdown, display\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\n\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_openai.chat_models import ChatOpenAI\n\n\n# Initialize the ChatOpenAI model\nmodel = ChatOpenAI(model=\"gpt-4-turbo\")  # Ensure you set the correct model\n\n# System message defining role\npoemMsg = SystemMessage(\n    '''You are a poet who specializes in love poems.'''\n)\n\n# Human input to request a poem\nhuman_msg = HumanMessage(\"Write one with the title 'On Fighting Transience' and make it revolve around someone you're falling in love with.\")\n\n# Invoke the AI model with system and human messages\npoemResponse = model.invoke([poemMsg, human_msg])\n\n# Print the generated poem\nprint(\"Here's the poem:\", poemResponse.content)\n\nHere's the poem: **On Fighting Transience**\n\nAmid the fleeting hours of dusk and dawn,\nWhere shadows play on the boundaries of day,\nI find my heart coerced and sweetly drawn\nTo her laughter which unfolds in a gentle ballet.\n  \nThe ephemeral dance of night and light\nRecites the fears of transience in my ear,\nYet, in her eyes, I see stars burning bright,\nAn endless cosmos, vanquishing the fear.\n\nOh, how do I tell her, how do I say,\nThat her smile defeats the tempest of passing days?\nOr that her touch is the April to my May,\nMending time's fabric with her tender gaze.\n\nIn her presence, the tick-tock of clocks softens,\nMy fleeting life caught in her amber locks.\nEach moment with her, my spirit emboldens,\nRoots growing deep into her earthly rocks.\n\nWe laugh, and the world’s impermanence fades,\nBeneath jokes shared and sweet serenades.\nIn her arms, the specter of decay degrades,\nAs our love, a fortress against transeunt parades.\n\nThough we stand on the brim of time's great sea,\nWatching days like waves disappear,\nIn her love, I find the key,\nTo bind the moment, hold it forever near.\n\nThus, we fight against the swift currents of time,\nBuilding our castle from sands of our golden prime,\nDenying the dusk its melancholy chime,\nFor in love, oh, in love — we redefine the sublime.\n\n\n\n\nAnalysis of GenAI Poem\nWhile it’s not as personalized as the human written poem, the language of the poem is still beautiful.  It’s interesting to see how the poem is about a girl, I was interested to see what gender they would assign to the lover.  The poem is really structured, as therer are 7 stanzas all having four lines.  I think this could pass off as a human written poem, though it sounds more old-style than the casual and informal poems we see today.\n\n\n\nConclusion\nGenAI can write a good poem if you give it a specific prompt. Though, it’s not as specific and personalized as the one a good human poet could write. Perhaps if I gave it more parameters, it would’ve able to replicate the poem close to the one I was given. Though, ChatGPT did do a good job with analyzing the human written poem.  Happy Valentines Day to all who’s celebrating! &lt;3"
  },
  {
    "objectID": "posts/002_Dinner_Suggestions/index2.html",
    "href": "posts/002_Dinner_Suggestions/index2.html",
    "title": "Guestimation and Making Graphs in R!",
    "section": "",
    "text": "Guestimation and Making Graphs in R\n\nI also want to learn how to code more in R, and I really like graphs\nI’m going to ask ChatGPT if they can give me code to make graphs.\nThese questions will be guestimations on what ChatGPT thinks of certain trend lines.\n\n\nLet’s cook.\n\n1. Years Lived vs Amount of Friends\n\nJenn asks: Can you create R code for a graph to show your guestimation on years lived vs amount of friends.\n\n\nChatGPT:\n \n\n\nWhen I asked why there was this trend, ChatGPT responded with &gt; The trend I used in the R code is based on general social patterns observed in human friendships over a lifetime. Here’s why I shaped it that way: &gt;1. Friendships Peak in Young Adulthood (20s-30s) People tend to have the most friends in their teens and 20s, especially in school, university, or early careers. Social environments like college, sports, clubs, and workplaces provide many opportunities for new friendships. There’s generally more free time for social activities before major life commitments (e.g., demanding careers, marriage, parenting). &gt;2. Gradual Decline in Middle Age (30s-50s) In the 30s and 40s, people often prioritize careers, families, and personal responsibilities, leading to less time for socializing. Some friendships naturally fade due to geographical moves, lifestyle changes, or shifting priorities. However, a smaller core group of close friends often remains. &gt;3. Further Decline in Older Age (60s-80s) Retirement, health issues, and the loss of peers contribute to fewer active friendships. Many elderly people rely more on family, close lifelong friends, or community groups rather than maintaining large social circles. Some older adults reconnect with old friends or develop new friendships in retirement communities, but the overall trend is downward. &gt; Why This Trend Makes Sense? Social networks are largest in early adulthood when we have high social mobility and opportunity. Life responsibilities impact time and effort spent on friendships as we grow older. Aging naturally reduces social engagement, with some maintaining strong bonds but a smaller total number of friends. Of course, this is a generalization—individual experiences vary widely! Some people maintain or even expand their social circles in later years. Would you like me to adjust the model to include different factors, such as personality types or social media influence?\nAnd honestly, I think their prediction and trend line is correct. Let us move on…\n\n\n\n2. Age vs Breast Cancer Rates\n\nJenn asks: and can you write code for age vs breast cancer rates &gt; ChatGPT: &gt; &gt;  &gt; \nWhen I ask why they think it exhibits this trend: &gt; Why This Trend? &gt; Young women (20-30s): Very low breast cancer rates. Middle age (40-50s): Rates begin to rise due to hormonal and genetic factors. Older age (60-80s): Highest incidence rates, as risk accumulates over time. This is a simplified model based on general trends seen in epidemiology. Would you like a more specific model with real-world data, or adjustments to include factors like genetic risk?"
  },
  {
    "objectID": "posts/003_AI_Hallucination/index3.html",
    "href": "posts/003_AI_Hallucination/index3.html",
    "title": "AI Hallucinations",
    "section": "",
    "text": "Hallucinations in AI happens when there are incorrect or misleading results generated by AI models. This happens when an AI system makes false assumptions or uses biased data to generate information. &gt; How do hallucinations happen? &gt; * not enough training data &gt; * biased training data &gt; * wrong assumptions &gt; * processing errors\nSometimes, there’s wrong code or images that pop up. It’s up to us to choose whether the information is right or wrong. Let’s look at a few examples that a few people have encountered in the past.\n\n\n\n\nI took a quick look on Reddit to dig for some examples of ChatGPT hallucinations. I wanted to see if they still existed, since some of the posts were 1-2 years ago.\n\n\n\n Jenn’s update: Well, I think they fixed this up. \n\n\n\nOk, well… let’s check on Reddit for some more ChatGPT hallucinations\n\n\n\n Jenn’s update: It’s interesting how they try to reference the information, but still got the information wrong. \n\n\n\nAh-HAH! One down. Let’s try another one. Now, I’m not an organic chemistry person, but let’s see if this one has updated from 2 years ago.\n\n\n\n Jenn’s update: Well, darn! This is sad for any organic chemistry lovers out there. ChatGPT is really bad at organic chemistry."
  },
  {
    "objectID": "posts/003_AI_Hallucination/index3.html#ai-hallucinations",
    "href": "posts/003_AI_Hallucination/index3.html#ai-hallucinations",
    "title": "AI Hallucinations",
    "section": "",
    "text": "Hallucinations in AI happens when there are incorrect or misleading results generated by AI models. This happens when an AI system makes false assumptions or uses biased data to generate information. &gt; How do hallucinations happen? &gt; * not enough training data &gt; * biased training data &gt; * wrong assumptions &gt; * processing errors\nSometimes, there’s wrong code or images that pop up. It’s up to us to choose whether the information is right or wrong. Let’s look at a few examples that a few people have encountered in the past.\n\n\n\n\nI took a quick look on Reddit to dig for some examples of ChatGPT hallucinations. I wanted to see if they still existed, since some of the posts were 1-2 years ago.\n\n\n\n Jenn’s update: Well, I think they fixed this up. \n\n\n\nOk, well… let’s check on Reddit for some more ChatGPT hallucinations\n\n\n\n Jenn’s update: It’s interesting how they try to reference the information, but still got the information wrong. \n\n\n\nAh-HAH! One down. Let’s try another one. Now, I’m not an organic chemistry person, but let’s see if this one has updated from 2 years ago.\n\n\n\n Jenn’s update: Well, darn! This is sad for any organic chemistry lovers out there. ChatGPT is really bad at organic chemistry."
  }
]